# -*- coding: utf-8 -*-
"""Hate_Speech_copy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1niYPiN1PTeEj1JF08Ici8XxsOfD_IbNw

# <h1 align = center> ***Impact  of nature of content on online hate speech detection*** </h1>

# <h2> *Librairies Downloading* </h2>
"""

!pip install emoji
!pip install fasttext
!pip install fasttext-wheel
!pip install gensim
!pip install graphviz
!pip install matplotlib
!pip install nltk
!pip install pandas
!pip install pydot
!pip install pydotplus
!pip install pyspellchecker
!pip install scikit-learn
!pip install seaborn
!pip install scikeras
!pip install tf-keras==2.15.1
!pip install transformers==4.37.2
!pip install xgboost
!pip install wordcloud

# !pip install transformers datasets huggingface_hub tensorboard==2.11
# # !sudo apt-get install git-lfs --yes
# !pip install --upgrade tensorflow
# !pip uninstall tensorflow -y
# !pip install tensorflow

import nltk
nltk.download('averaged_perceptron_tagger')
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

import logging
logging.getLogger("transformers.tokenization_utils_base").setLevel(logging.ERROR)

from google.colab import drive
drive.mount('/content/drive')

"""# <h2> *Librairies import* </h2>"""

#For data analysis
import math
import numpy as np
import pandas as pd
import tensorflow as tf
import time

#For data visualization
import matplotlib.pyplot as plt
from matplotlib.ticker import PercentFormatter
import seaborn as sns
from tqdm import tqdm
from wordcloud import STOPWORDS,WordCloud


#For data preprocessing
import re
import string
import emoji
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tag import pos_tag
from nltk.tokenize import TweetTokenizer
from nltk.tokenize import word_tokenize
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.layers import Layer, Attention
from tensorflow.keras.layers import TextVectorization
from tensorflow.keras.preprocessing.sequence import pad_sequences
# from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer


#For evaluation metrics and grid search
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from scikeras.wrappers import KerasClassifier
from sklearn.metrics import (
     accuracy_score, auc, classification_report, confusion_matrix, f1_score,
     precision_recall_curve, roc_auc_score, precision_score, recall_score,
     roc_curve)
from sklearn.model_selection import GridSearchCV, train_test_split, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras import losses
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.regularizers import l1, l2


# For text vectorization and word embedding
import fasttext
from gensim.models import FastText
#from tensorflow.keras.layers.experimental.preprocessing import TextVectorization
# from spellchecker import SpellChecker
#For ML and DL models
# ML Models
import xgboost as xgb
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression, SGDClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC, LinearSVC
from xgboost.sklearn import XGBClassifier

#DL Models
from tensorflow.keras.initializers import Constant
from tensorflow.keras.layers import (
     Bidirectional, concatenate, Conv1D, Dense, Dropout, Embedding, Flatten,
    GlobalAveragePooling1D, GlobalMaxPooling1D, GRU, Input, MaxPooling1D,
     LSTM, SpatialDropout1D)
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.utils import plot_model

# import os

# os.environ["SM_FRAMEWORK"] = "tf.keras"
import transformers
from transformers import AutoModelForSequenceClassification, TFRobertaModel, TFBertForSequenceClassification, TFAutoModelForSequenceClassification, TFRobertaForSequenceClassification, TFBartForSequenceClassification, BartTokenizer, TrainingArguments, Trainer, create_optimizer, DataCollatorWithPadding
# from datasets import load_dataset
from transformers import ( AutoTokenizer,
    RobertaModel, RobertaTokenizer,
    RobertaTokenizerFast,
    RobertaForSequenceClassification,
    TrainingArguments,
    Trainer,
    AutoConfig,
)
from huggingface_hub import HfFolder, notebook_login

print(tf.__version__)

"""# <h2>*Functions*</h2>"""

####################################################################################################For Data pre processing#############################################################################################################

# def check_misspelled(text):
#   output_list = []
#   output_list_to_text = []
#   spell = SpellChecker()
#   w = spell.unknown(text.split())
#   for i in text.split():
#     if i not in w:
#       output_list.append(i)
#     else:
#       if (not spell.correction(i)):
#         output_list.append(i)
#         output_list_to_text.append(i)
#       else:
#         output_list.append(spell.correction(i))
#   result = ' '.join(output_list)
#   file1 = open('myFile1.txt','w')
#   file1.writelines(output_list_to_text)
#   file1.close()
#   return result

def decontracted(text):
    # specific
    text = re.sub(r"won\'t", "will not", text)
    text = re.sub(r"i\'m", "i am", text)
    text = re.sub(r"can\'t", "can not", text)
    text = re.sub(r"ain\'t", "am not", text)
    text = re.sub(r"arent",  "are not", text)
    text = re.sub(r"aren\'t", "are not", text)
    text = re.sub(r"dont", "do not", text)
    text = re.sub(r"didnt", "did not", text)
    text = re.sub(r"havent", "have not", text)
    text = re.sub(r"haven\'t", "have not", text)
    text = re.sub(r"shouldnt", "should not", text)
    text = re.sub(r"shouldn\'t", "should not",text)
    text = re.sub(r"shan\'t", "shall not", text)
    text = re.sub(r"sha\'n\'t", "shall not", text)
    text = re.sub(r"let\'s", "let us", text)
    text = re.sub(r"o\'clock", "of the clock", text)
    text = re.sub(r"tv", "television", text)
    text = re.sub(r"b!ack", "black", text)
    text = re.sub(r"wont", "will not", text)
    text = re.sub(r"cant", "can not", text)
    text = re.sub(r"aint", "am not", text)
    text = re.sub(r"shant", "shall not", text)
    text = re.sub(r"ma\'am", "madam", text)
    text = re.sub(r"y\'all", "you all", text)
    # general
    text = re.sub(r"n\'y", " you", text)
    text = re.sub(r"n\'t", " not", text)
    text = re.sub(r"\'re", " are", text)
    text = re.sub(r"\'s", " is", text)
    text = re.sub(r"\'d", " would", text)
    text = re.sub(r"\'ll", " will", text)
    text = re.sub(r"\'t", " not", text)
    text = re.sub(r"\'ve", " have", text)
    text = re.sub(r"\'m", " am", text)
    return text

def lemmatize(text):
    lemmatizer = WordNetLemmatizer()
    output = []
    for word, tag in pos_tag(text.split()):
        if tag.startswith("NN"): #Noun
            output.append(lemmatizer.lemmatize(word,pos='n'))
        elif tag.startswith("VB"): #Verb
            output.append(lemmatizer.lemmatize(word,pos='v'))
        elif tag.startswith("JJ") | tag.startswith("VB") : #Adjective and adverb
            output.append(lemmatizer.lemmatize(word,pos='s'))
        else:
            output.append(lemmatizer.lemmatize(word))
    final_output = " ".join(output)
    return final_output

def remove_punctuation(text):
    translator = str.maketrans("", "", string.punctuation)
    result = text.translate(translator)
    return result

def remove_stopwords(text):
    stop_words = set(stopwords.words('english'))
    filtered_output = [u for u in text if u not in stop_words]
    return filtered_output

def text_has_emoji(text):
    tmp = ''
    tmp_list = []
    for character in text:
        if character in emoji.EMOJI_DATA:
            tmp = emoji.replace_emoji(character, replace =
                                      emoji.demojize(character,language='en'))
        else:
            tmp = character
        tmp_list.append(tmp)
        final_output = " ".join(tmp_list)
    return final_output

def translate_character(text):
    translator = str.maketrans("@340", "aeao")
    result = text.translate(translator)
    return result

def preprocess_text(text):
    # %%time
    start = time.time()
    tqdm.pandas()
    # spell = SpellChecker()
    # Lower case all the words before data preprocessing
    text = text.str.lower()
    # Short version terms
    text = text.progress_apply(lambda text: decontracted(text))
    #Character translation
    text = text.progress_apply(lambda text: translate_character(text))
    #Tokenization
    tokenizer = TweetTokenizer()
    text = text.progress_apply(lambda text: tokenizer.tokenize(text))
    # Stopwords removing
    text = text.progress_apply(lambda text: remove_stopwords(text))
    #Replacing Emojis with their semantic equivalent
    text = text.progress_apply(lambda text: text_has_emoji(text))
    # # Punctuation removing
    text = text.progress_apply(lambda text: remove_punctuation(text))
    # # Lemmatization
    text = text.progress_apply(lambda text: lemmatize(text))
    # Check misspeled words
    # new_df['misspelled_words'] = new_df.text.progress_apply(lambda text:
                                #spell.unknown(str(text).split()))
    # new_df['misspelled_words'].to_csv('output.txt', index=False, header=False)

    end = round(time.time()-start,2)
    print("This process took",end,"seconds.")
    return text

################################################################################################# FOR MODEL BUILDING ###################################################################################################################
MAX_SEQUENCE_LENGTH = 300
# Emebdding matrix building
def build_embedding_matrix(vocab, emb_model, emb_dim, verbose = False):
    hits = 0
    misses = 0
    vocab_dict = dict(zip(vocab, range(len(vocab))))
    # Vocabulary size
    vocab_size = len(vocab_dict) + 1
    # Embedding matrix initialization
    embedding_matrix = np.zeros((vocab_size, emb_dim))
    if isinstance(emb_model, dict): # GloVE
        for word,i in tqdm(vocab_dict.items()):
            embedding_value = emb_model.get(word)
            if embedding_value is not None:
                embedding_matrix[i] = embedding_value
                hits += 1
            else:
                misses += 1
    elif isinstance(emb_model, FastText): # FastText gensim
        for word, i in tqdm(vocab_dict.items()):
            try:
                embedding_vector = emb_model.wv[word]
            except:
                print(word, 'not found')
            if embedding_vector is not None:
                embedding_matrix[i, :] = embedding_vector
                hits += 1
            else:
                misses += 1
    if verbose is True:
        print("Converted %d words (%d misses)" % (hits, misses))
    return embedding_matrix

# Analyzes the data and builds the vocabulary
def build_vocabulary(doc, max_tokens_, out_seq_length):
     text_vector = TextVectorization(max_tokens= max_tokens_,
                                     output_sequence_length=out_seq_length)
     text_vector.adapt(doc)
     return text_vector

# Compute ROC curve and area under the curve
def draw_roc_curve(model, x_test, y_test_):
    print(f"The best model is : {type(model)}")
    pred = model.predict(x_test)
    fpr, tpr, thresholds = roc_curve(y_test_, pred)
    roc_auc = auc(fpr, tpr)
    # Plot ROC curve
    plt.figure()
    lw = 2
    plt.plot(fpr, tpr, color='darkorange', lw=lw,
             label='ROC curve (area = %0.3f)' % roc_auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc="lower right")
    plt.show()

# Integer encode and pad documents
def encode_text(text, text_vectorizer, max_length):
    # integer encode
    encoded_data = text_vectorizer(text).numpy()
    # pad sequences
    padded = pad_sequences(encoded_data, maxlen = max_length, padding = 'post')
    return padded

# Evaluate ML Models
def evaluate_model_ml(dict,train,dev,y_train_,y_dev_):
    accuracy_dict ={}
    for model_key, model_value in dict.items():
        model_value.fit(train,y_train_)
        y_pred_model = model_value.predict(dev)
        print(f"\t {model_key} Report")
        accuracy = round(accuracy_score(y_dev_,y_pred_model),3)
        accuracy_dict[model_key] = accuracy
        print(f" Accuracy of {model_key} is: {accuracy}")
        print(f""" Roc score of {model_key} is:
              {round(roc_auc_score(y_dev, y_pred_model),3)}""")
        print(classification_report(y_dev_,y_pred_model))
    max_key = max(accuracy_dict, key=accuracy_dict.get)
    best_model = dict[max_key]
    return best_model

def evaluate_model_ml_cv(dict, train, dev, test, y_train_, y_dev_, y_test_):
    accuracy_dict ={}
    f1_dict ={}
    roc_dict ={}

    for model_key, model_value in dict.items():
        # Accuracy
        print("----------------------------------------------------------------")
        print(f"{model_key} case")
        print("----------------------------------------------------------------")
        accuracy_scores = cross_val_score(model_value, train, y_train_, cv=3, scoring='accuracy')
        rounded_acc_scores = [round(score, 3) for score in accuracy_scores]
        print("Cross-validation scores:", rounded_acc_scores)
        print("Average cross-validation accuracy score:", round(accuracy_scores.mean(), 3))
        print("\n")
        # F1-score
        f1_scores = cross_val_score(model_value, train, y_train_, cv=3, scoring='f1_macro')
        rounded_f1_scores = [round(score, 3) for score in f1_scores]
        print("Cross-validation scores:", rounded_f1_scores)
        print("Average cross-validation f1-score:", round(f1_scores.mean(), 3))
        print("\n")
        # AUC ROC
        roc_scores = cross_val_score(model_value, train, y_train_, cv=3, scoring='roc_auc')
        rounded_roc_scores = [round(score, 3) for score in roc_scores]
        print("Cross-validation scores:", rounded_roc_scores)
        print("Average cross-validation roc-auc score:", round(roc_scores.mean(), 3))
        print("\n")

        # Train the model on the training set
        model_value.fit(train,y_train_)
        # Evaluate the model on the development set
        y_pred_dev = model_value.predict(dev)

        dev_accuracy = round(accuracy_score(y_dev_, y_pred_dev), 3)
        dev_f1 = round(f1_score(y_dev, y_pred_dev, average='macro'), 3)
        dev_auc = round(roc_auc_score(y_dev, y_pred_dev), 3)
        print("Development set accuracy:", dev_accuracy)
        print("Development set F1-score:", dev_f1)
        print("Development set AUC-ROC:", dev_auc)
        print("\n")
        print("----------------------------------------------------------------")
        # Evaluate the model on the test set
        print(f"\t {model_key} Report")
        y_pred_test = model_value.predict(test)
        test_accuracy = round(accuracy_score(y_test_, y_pred_test), 3)
        test_f1 = round(f1_score(y_test, y_pred_test, average='macro'), 3)
        test_auc = round(roc_auc_score(y_test, y_pred_test), 3)

        print(f" Test Accuracy of {model_key} is: {test_accuracy}")
        print(f" Test F1-score of {model_key} is: {test_f1}")
        print(f" Test AUC-ROC of {model_key} is: {test_auc}")



        accuracy_dict[model_key] = test_accuracy
        f1_dict[model_key] = test_f1
        roc_dict[model_key] = test_auc

        print("----------------------------------------------------------------")
        print(classification_report(y_test_,y_pred_test))
        print("----------------------------------------------------------------")
    max_key = max(accuracy_dict, key=accuracy_dict.get)
    best_model = dict[max_key]
    return best_model



# Load pre-trained GloVe embeddings
def load_embeddings(file_path):
    embeddings = {}
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in tqdm(f):
            values = line.split()
            word = values[0]
            vector = np.asarray(values[1:], dtype='float32')
            embeddings[word] = vector
    return embeddings

# Vectorize text data
def vectorize_text(text, emb_model, emb_dim, max_seq, average = False):
    # First we need to split the text into its tokens and learn the length
    # If length is shorter than the max len we'll add some spaces (emb_dim D
    # vectors which has only zero values)
    # If it's longer than the max len we'll trim from the end.
    words = text.split()
    len_v = len(words)-1 if len(words) < max_seq else max_seq-1
    vectors = []
    if isinstance(emb_model,FastText): # FastText gensim
        vectors = [emb_model.wv[word] for word in words[:len_v] if word in
                   emb_model.wv]
    else: # GloVE
        vectors = [emb_model.get(word) for word in words[:len_v] if
                   emb_model.get(word) is not None]
    last_pieces = max_seq - len(vectors)
    for i in range(last_pieces):
        vectors.append(np.zeros(emb_dim,))

    if average is True:
        return np.asarray(np.mean(vectors, axis=0)).flatten()
    else:
        return np.asarray(vectors).flatten()
################################################################################################ DL Models ##########################################################################################

# precision-recall
def draw_auprc_score(model, X_test, y_test):
    prediction_probs = model.predict(X_test, verbose=1).ravel()
    precision, recall, thresholds = precision_recall_curve(y_test, prediction_probs)
    print(f'model AUPRC score: {auc(recall, precision)}')
    plt.plot(precision, recall)
    plt.xlabel('Recall')
    plt.ylabel('Precision')

# confusion matrix
def draw_confusion_matrix(model, X_test, y_test):
    prediction_probs = model.predict(X_test, verbose=1).ravel()
    predictions = [1 if prob > 0.5 else 0 for prob in prediction_probs]
    labels = ['hate', 'no-hate']
    print(classification_report(y_test, predictions))
    return pd.DataFrame(confusion_matrix(y_test, predictions), index=labels, columns=labels)

# evaluate model
def evaluate_model(model, X_test, y_test):
    test_loss, test_accuracy, test_precision, test_recall = model.evaluate(X_test, y_test, verbose=3)
    # get other metrics
    y_pred = model.predict(X_test)
    y_pred = (y_pred > 0.5).astype(int) # Convert probabilities to binary predictions
    precision_test = precision_score(y_test, y_pred , average="macro")
    recall_test = recall_score(y_test, y_pred , average="macro")
    f1_score_test = f1_score(y_test, y_pred , average="macro")
    return test_loss,test_accuracy, precision_test, recall_test, f1_score_test

# fit network
def fit_network(model, X_train, y_train, X_val, y_val):
    name = model.name
    # Create the filepath using f-string
    filepath = f"{name}.keras"
    mc = ModelCheckpoint(filepath, monitor='val_accuracy', mode='max', verbose=3, save_best_only=True)
    callbacks_list = [EarlyStopping(
                  monitor='val_loss',
                  restore_best_weights=True,
                  patience=4)]
    history = model.fit(X_train, y_train,
                    epochs=25,
                    batch_size= 64,
                    callbacks = [mc,callbacks_list],
                    validation_data=(X_val,y_val)
                    )
    return history

# AUC ROC score
def get_auc_score(model, X_test, y_test):
    prediction_probs = model.predict(X_test, verbose=1).ravel()
    print(f'model AUC score: {roc_auc_score(y_test, prediction_probs)}')


def predict_label(text, text_vectorizer, max_length, model):
    # clean review
    text_clean = preprocess_text(text)
    # encode and pad text
    padded = encode_text(text_vectorizer, max_length, text_clean)
    # predict label
    yhat = model.predict(padded, verbose=0)
    # retrieve predicted percentage and label
    percent_pos = yhat[0,0]
    if round(percent_pos) == 0:
        return (1-percent_pos), 'HATE'
    return percent_pos, 'NON-HATE'

def plot_accuracy_loss(model_history):
    accuracy = model_history.history['accuracy']
    val_accuracy = model_history.history['val_accuracy']
    loss = model_history.history['loss']
    val_loss = model_history.history['val_loss']
    score = np.mean(val_accuracy)
    # score = max(val_accuracy)
    print('Average Accuracy score:',score)
    print('Best loss:',min(val_loss))
    best_epoch = np.argmin(val_loss)
    print('for epoch :',best_epoch)

    epochs = range(len(loss))

    plt.plot(epochs, accuracy, label='Training accuracy')
    plt.plot(epochs, val_accuracy, label='Validation accuracy')
    plt.axvline(x=best_epoch, linestyle='--', color='r', label='Best Model')
    plt.title('Training and validation accuracy score')
    plt.legend()

    plt.figure()

    plt.plot(epochs, loss, label='Training loss')
    plt.plot(epochs, val_loss, label='Validation loss')
    plt.axvline(x=best_epoch, linestyle='--', color='r', label='Best Model')
    plt.title('Training and validation loss')
    plt.legend()
    plt.show()

################################################################################################ Transformers ##########################################################################################
# BERT
# ROBERTa
# Defining some key variables that will be used later on in the training
MAX_LEN = 100
TRAIN_BATCH_SIZE = 8
VALID_BATCH_SIZE = 4
EPOCHS = 3
LEARNING_RATE = 1e-05
MODEL_NAME = 'roberta-base'

strategy = tf.distribute.get_strategy()

print('Number of replicas:', strategy.num_replicas_in_sync)

def roberta_encode(texts, tokenizer):
    ct = len(texts)
    input_ids = np.ones((ct, MAX_LEN), dtype='int32')
    attention_mask = np.zeros((ct, MAX_LEN), dtype='int32')
    token_type_ids = np.zeros((ct, MAX_LEN), dtype='int32') # Not used in text classification

    for k, text in enumerate(texts):
        # Tokenize
        tok_text = tokenizer.tokenize(text)

        # Truncate and convert tokens to numerical IDs
        enc_text = tokenizer.convert_tokens_to_ids(tok_text[:(MAX_LEN-2)])

        input_length = len(enc_text) + 2
        input_length = input_length if input_length < MAX_LEN else MAX_LEN

        # Add tokens [CLS] and [SEP] at the beginning and the end
        input_ids[k,:input_length] = np.asarray([0] + enc_text + [2], dtype='int32')

        # Set to 1s in the attention input
        attention_mask[k,:input_length] = 1

    return {
        'input_word_ids': input_ids,
        'input_mask': attention_mask,
        'input_type_ids': token_type_ids
    }

# def fix_length(tokens, max_sequence_length=100):
#     length = len(tokens)
#     if length > max_sequence_length:
#         tokens = tokens[:max_sequence_length-1]
#     return tokens

# # function for tokenizing the input data for transformer.
# def transformer_inputs(text,tokenizer,MAX_SEQUENCE_LENGTH = 100):
#     text_tokens = tokenizer.tokenize(str(text))
#     text_tokens = fix_length(text_tokens)
#     ids_q = tokenizer.convert_tokens_to_ids(["[CLS]"] + text_tokens)
#     padded_ids = (ids_q + [tokenizer.pad_token_id] * (MAX_LEN - len(ids_q)))[:MAX_LEN]
#     #token_type_ids = ([0] * MAX_SEQUENCE_LENGTH)[:MAX_SEQUENCE_LENGTH]
#     attention_mask = ([1] * len(ids_q) + [0] * (MAX_LEN - len(ids_q)))[:MAX_LEN]

#     return padded_ids,attention_mask

# # function for creating the input_ids, masks and segments for the bert input
# def input_for_model(texts, tokenizer):
#     print(f'generating input for transformer...')
#     input_ids,input_attention_masks = [], []
#     for text in texts:
#         ids, mask = transformer_inputs(text,tokenizer)
#         input_ids.append(ids)
#         input_attention_masks.append(mask)

#     return (
#         np.asarray(input_ids, dtype=np.int32),
#         np.asarray(input_attention_masks, dtype=np.int32))

# def compute_output_arrays(df, columns):
#     return np.asarray(df[columns])

# Grid Search CV for DL models
# We tune hyperparameters step by step to avoid misconfigurations and
# misunderstanding during debuggage
# l1_l2 = [(0, 0), (0.01, 0), (0, 0.01)]
# optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam',
# 'Adamax', 'Nadam']
# Adadelta does not support parameter "learning_rate"
# learning_rate = [0.00001, 0.00003, 0.0001, 0.0003, 0.001, 0.003, 0.01,
# 0.03, 0.1, 0.3]
# activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh',
#'sigmoid', 'hard_sigmoid', 'linear']


# Batch size and number of epochs
# def tune_batch_size_epochs(model, X_train, y_train):
#     keras_model = KerasClassifier(model, verbose = 3)
#     # define grid parameters
#     batch_size = [16, 32, 64, 128, 256]
#     epochs = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
#     param_grid = dict(keras_model__batch_size = batch_size, keras_model__epochs = epochs)
#     grid = GridSearchCV(estimator = keras_model, param_grid = param_grid, cv = 5, verbose = 3)
#     grid_result = grid.fit(X_train, y_train)
#     # summarize results
#     print(f" Best result is : {grid_result.best_score_} using {grid_result.best_params_}")
#     return grid_result.best_params_

# # Optimization algorithm
# def tune_optimization_algorithm(model, X_train, y_train, batch_epoch):
#     batch_size = batch_epoch.get(0)
#     epochs = batch_epoch.get(1)
#     keras_model = KerasClassifier(model, verbose = 3,
#                                   keras_model__epochs = epochs,
#                                   keras_model__batch_size = batch_size)
#     # define grid parameters
#     optimization_function = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']
#     param_grid = dict(keras_model__optimizer = optimization_function)
#     grid = GridSearch(estimator = keras_model, param_grid = param_grid, cv = 5, verbose = 3)
#     grid_result = grid.fit(X_train, y_train)
#     print(f"Best result is: {grid_result.best_score_} using {grid_result.best_params_}")
#     return grid_result.best_params_

# # Learning rate
# def tune_learning_rate(model, X_train, y_train, optimizer, batch_epoch, optimizer):
#     optimizer = optimizer.get(0)
#     batch_size = batch_epoch.get(0)
#     epochs = batch_epoch.get(1)
#     keras_model = KerasClassifier(model, verbose = 3, keras_model__epochs = epochs,
#                                   keras_model__batch_size = batch_size,
#                                   keras_model__optimizer = optimizer)
#     # define grid parameters
#     learning_rate = [0.00001, 0.00003, 0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3]
#     # momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]
#     param_grid = dict(learning_rate = learning_rate, momentum = momentum)
#     grid = GridSearch(estimator = keras_model, param_grid = param_grid, cv = 5, verbose = 3)
#     grid_result = grid.fit(X_train, y_train)
#     print(f"Best result is: {grid_result.best_score_} using {grid_result.best_params_}")
#     return grid_result.best_params_

"""# <h2> *Data understanding and preparation* </h2>"""

#Read the dataset
df = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/Dynamically Generated Hate Dataset v0.2.3.csv")

#Get info about the dataset
df.info()

df.drop(columns='Unnamed: 0', inplace=True)

#Let's determine the null rows
#It appears that both "target" and "acl.id.matched" have null rows.
df.loc[df['target'].isnull()]

#For 2-class task and 5-class task, we are not focused on target column so we can keep all rows from this column

df.loc[df['acl.id.matched'].isnull()]

#Get a view of the data
df.head()

df[(df['round.base'] == 2) & (df['label'] == 'hate')].head(50)

#Get the percentage of label and type hate
sns.set_theme()

fig, (ax1,ax2) = plt.subplots(ncols=2,figsize=(18,8))
df.groupby('label').size().plot(kind='pie',
                                autopct=lambda p: '{:.2f}% ({:.0f})'.format(p,
                                 (p/100)*len(df)),
                                figsize=[14,14],
                                textprops={'fontsize': 12},
                                colors=['tomato', 'skyblue'],
                                explode=(0.1, 0),
                                shadow=True,
                                startangle=0,
                                ax =ax1
                                )
# ax2 = plt.subplot(121)
df[df.label == 'hate'].groupby('type').size().plot(kind = 'pie',
                               autopct = lambda p :'{:.2f}% ({:.0f})'.format(p,
                                (p/100)*len(df[df['label'] == 'hate'])),
                               figsize =[14,14],
                               textprops = {'fontsize': 12},
                               #explode=(0,0,0,0.1,0,0),
                               shadow= True,
                               startangle=0,
                               ax =ax2)
ax1.set_ylabel('Per label')
ax2.set_ylabel('Per type hate')
plt.tight_layout()
# plt.show()

#We will remove from hate dataset the rows with the type "notgiven"
#However this rows could be used in Semi Supervised Learning approach for further researches in order to build robust classifiers

#Get the count of target items
df[df['label']=='hate']['target'].value_counts()

#We count characters and word occurrences to establish a possible correlation between text length and no hate/ hate(type)
df['character_length'] = (df.text.str.len() - (df.text.str.split().str.len()-1))
df['word_length'] = df.text.str.split().str.len()

#Histplot of word and character occurrences
fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12,8))
sns.histplot(data = df, x = 'word_length',ax = ax[0,0], weights =
             np.ones(len(df)) / len(df),  bins= 35,edgecolor='black')
sns.histplot(data = df, x = 'character_length', ax = ax[0,1], weights =
             np.ones(len(df)) / len(df), bins = 35,edgecolor='black')
sns.histplot(data = df, x = 'word_length', ax = ax[1,0], hue='label',weights =
             np.ones(len(df)) / len(df), bins= 20,edgecolor='black')
sns.histplot(data = df, x = 'character_length',ax = ax[1,1],  hue ='label',
             weights=np.ones(len(df)) / len(df), bins = 20,edgecolor='black')


ax[0,0].set_title('Distribution of word occurrences')
ax[0,1].set_title('Distribution of character occurrences')
ax[1,0].set_title('Distribution of word occurrences per label')
ax[1,1].set_title('Distribution of character occurrences per label')

all_axes = plt.gcf().get_axes()
for ax in all_axes:
    ax.set_ylabel('Text')
    ax.yaxis.set_major_formatter(PercentFormatter(1, decimals=0))
plt.tight_layout()
plt.show()

len(df.target.dropna().unique())

#We observe that the majority of sentences are texts with less than 150 words and 600 characters
#The hate and non-hate labels have similarly the same behavior regarding the word and character length
#However we can denote that the hate label has more sentences. For e.g, ~28% of not hate sentences contain words with a length between 1 and 25
#Compared to ~35% of hate sentences

# To create robust classifiers we remove outliers and re-define maximum sizes for characters and words
#Max word size = 200
#Max character = 600
new_df = df.loc[(df['word_length'] <=100) | (df['character_length']<=500)]

len(df)

new_df.info()

#Histplot of word and character occurrences
fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(12,8))
sns.histplot(data = new_df, x = 'word_length',ax = ax[0,0],
             weights=np.ones(len(new_df)) / len(new_df),
             bins= 35,edgecolor='black')
sns.histplot(data = new_df, x = 'character_length', ax = ax[0,1],
             weights=np.ones(len(new_df)) / len(new_df),
             bins = 35,edgecolor='black')
sns.histplot(data = new_df, x = 'word_length', ax = ax[1,0], hue='label',
             weights=np.ones(len(new_df)) / len(new_df),
             bins= 20,edgecolor='black')
sns.histplot(data = new_df, x = 'character_length',ax = ax[1,1],  hue ='label',
             weights=np.ones(len(new_df)) / len(new_df),
             bins = 20,edgecolor='black')


ax[0,0].set_title('Distribution of word occurrences')
ax[0,1].set_title('Distribution of character occurrences')
ax[1,0].set_title('Distribution of word occurrences per label')
ax[1,1].set_title('Distribution of character occurrences per label')

all_axes = plt.gcf().get_axes()
for ax in all_axes:
    ax.set_ylabel('Text')
    ax.yaxis.set_major_formatter(PercentFormatter(1, decimals=0))
plt.tight_layout()
plt.show()

#Let's establish word and character distribution for hate types
fig, ax = plt.subplots(nrows = 2, ncols=2, figsize=(16,9))
#Word
sns.boxplot(data=new_df[(new_df.label == 'hate') & (new_df.type != 'notgiven')],
            x ='type',y= 'word_length', ax = ax[0,0],color='gold')
sns.barplot(data=new_df[(new_df.label == 'hate') & (new_df.type != 'notgiven')],
                 x ='type',y= 'word_length',err_kws={'linewidth': 0},
                 color='gold',
                 ax = ax[0,1])
#Character
sns.boxplot(data=new_df[(new_df.label == 'hate') & (new_df.type != 'notgiven')],
            x ='type',y= 'character_length', ax = ax[1,0], color = 'green')
sns.barplot(data=new_df[(new_df.label == 'hate') & (new_df.type != 'notgiven')],
                 x ='type',y= 'character_length',err_kws={'linewidth': 0},
                 color='green',
                 ax = ax[1,1])
for i in ax[0,1].containers:
    ax[0,1].bar_label(i,)
for i in ax[1,1].containers:
    ax[1,1].bar_label(i,)

#We still denote some outliers for all types hate and a linear correlation between character and word observations
# Animosity and Dehumanization types seem to have the highest word and character length, and the threatening type the least one.

#Training and Testing dataset for 2-class and 5-class
sns.set_theme()
fig, (ax1,ax2)= plt.subplots(ncols=2,figsize=(14,4))
new_df.groupby('level').size().plot(kind = 'pie',
                              autopct = lambda p: '{:.2f}% ({:.0f})'.format(p,
                              (p/100)*len(new_df)),
                              figsize = [10,10],
                              textprops = {'fontsize': 12},
                              shadow = True,
                              startangle =0,
                              ax = ax1)
new_df[(new_df['label']=='hate') & (new_df.type != 'notgiven')].groupby('level').size().plot(kind = 'pie',
                               autopct = lambda p :'{:.2f}% ({:.0f})'.format(p,
                                (p/100)*len(new_df[new_df['label'] == 'hate'])),
                               figsize =[10,10],
                               textprops = {'fontsize': 12},
                               #explode=(0,0.1),
                               shadow= True,
                               startangle=0,
                               ax =ax2)
ax1.set_title('Dataset distribution for training/testing : 2-class task')
ax2.set_title('Dataset distribution for training/testing : 5-class task')
plt.tight_layout()
plt.show()

#Training and Testing dataset for 2-class and 5-class
sns.set_theme()
fig, (ax1,ax2)= plt.subplots(ncols=2,figsize=(14,4))
new_df[new_df['level'] == 'original'].groupby('label').size().plot(kind = 'pie',
                              autopct = lambda p: '{:.2f}% ({:.0f})'.format(p,
                              (p/100)*len(new_df[new_df['level'] == 'original'])),
                              figsize = [10,10],
                              textprops = {'fontsize': 12},
                              shadow = True,
                              startangle =0,
                              ax = ax1)
new_df[new_df['level'] == 'perturbation'].groupby('label').size().plot(kind = 'pie',
                               autopct = lambda p :'{:.2f}% ({:.0f})'.format(p,
                                (p/100)*len(new_df[new_df['level'] == 'perturbation'])),
                               figsize =[10,10],
                               textprops = {'fontsize': 12},
                               #explode=(0,0.1),
                               shadow= True,
                               startangle=0,
                               ax =ax2)
ax1.set_title('Dataset distribution for hate/no-hate : original level')
ax2.set_title('Dataset distribution for hate/no-hate : perturbation level')
plt.tight_layout()
plt.show()

#Word cloud of dataset
# # Extract the text data
text = ' '.join(new_df.text)

# Generate the word cloud
wordcloud = WordCloud(width=800, height=800,
                      background_color='white',
                      stopwords=STOPWORDS,
                      min_font_size=10).generate(text)

# Plot the word cloud
plt.figure(figsize=(8, 8), facecolor=None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

"""# <h2> *Data pre-processing* </h2>"""

new_df.text = preprocess_text(new_df.text)
new_df.character_length = (new_df.text.str.len() -
                        (new_df.text.str.split().str.len() - 1))
new_df.word_length = new_df.text.str.split().str.len()
new_df.head()

# end = round(time.time()-start,2)
# print("This process took",end,"seconds.")

#Word cloud of dataset
# Extract the text data
text = ' '.join(new_df.text)
# Generate the word cloud
wordcloud = WordCloud(width=800, height=800,
                      background_color='white',
                      stopwords=STOPWORDS,
                      min_font_size=10).generate(text)

# Plot the word cloud
plt.figure(figsize=(8, 8), facecolor=None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

#Label ratio by level
sns.set_theme()
fig, (ax1,ax2)= plt.subplots(ncols = 2, figsize=(5,5))
new_df[(new_df['level']=='original')].groupby('label').size().plot(kind = 'pie',
                               autopct = lambda p :'{:.2f}% ({:.0f})'.format(p,(p/100)*len(new_df[new_df['level'] == 'original'])),
                               figsize = [10,10],
                               textprops = {'fontsize': 12},
                               #explode=(0,0.1),
                               shadow= True,
                               startangle=0,
                               ax = ax1
                               )
new_df[(new_df['level']=='perturbation')].groupby('label').size().plot(kind =
                                'pie',
                               autopct = lambda p :'{:.2f}% ({:.0f})'.format(p,
                              (p/100)*len(new_df[new_df['level'] ==
                              'perturbation'])),
                               figsize = [10,10],
                               textprops = {'fontsize': 12},
                               #explode=(0,0.1),
                               shadow= True,
                               startangle=0,
                               ax = ax2
                               )
ax1.set_title('Label original ratio by level')
ax2.set_title('Label perturbation ratio by level ')
plt.tight_layout()
plt.show()

"""# <h2> *Data splitting* </h3>"""

# ######2-class case######
# X_train = new_df[new_df.level == 'original']['text']
# y_train = new_df[new_df.level == 'original']['label']

# perturbation_data_text = new_df[new_df.level == 'perturbation']['text']
# perturbation_data_label = new_df[new_df.level == 'perturbation']['label']

# X_train = new_df[new_df.level == 'perturbation']['text']
# y_train = new_df[new_df.level == 'perturbation']['label']

# original_data_text = new_df[new_df.level == 'original']['text']
# original_data_label = new_df[new_df.level == 'original']['label']
# Data split


# #####Multi-label case####

# X = new_df[(new_df['label']=='hate') & (new_df.type != 'notgiven')]['text']
# y = new_df[(new_df['label']=='hate') & (new_df.type != 'notgiven')]['type']

# Splitting
X_train, X_dev, y_train, y_dev = train_test_split(new_df.text, new_df.label, test_size=0.4, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
# Multi-task case
# X_train_mult, X_dev_mult, y_train_mult, y_dev_mult = train_test_split(X, y, test_size = 0.3, random_state=42)
# X_train_mult, X_test_mult, y_train_mult, y_test_mult = train_test_split(X_train_mult, y_train_mult, test_size = 0.2, random_state=42)

# X_dev, X_test, y_dev, y_test = train_test_split(original_data_text,
#                                                 original_data_label,
#                         #perturbation_data_label,
#                         test_size = 0.25,
#                         random_state = 64)

# X_dev, X_test, y_dev, y_test = train_test_split(perturbation_data_text,
#                                                 perturbation_data_label,
#                         #perturbation_data_label,
#                         test_size = 0.8,
#                         random_state = 42)

#create the transforms
tf_idf_vectorizer = TfidfVectorizer(max_features= 20000, max_df=0.5,
                                    ngram_range= (1,2), use_idf=False,
                                    norm="l1")
# vectorizer_original = TfidfVectorizer()
# vectorizer_perturbation = TfidfVectorizer()

#tokenize and build vocabulary
#tf_idf_vectorizer.fit_transform(new_df.text)
#vectorizer_original.fit(original_data_text)
# encode the document
X_train_tf_idf = tf_idf_vectorizer.fit_transform(X_train)

#tokenize and build vocabulary
#vectorizer_perturbation.fit(perturbation_data_text)
#encode the document
X_dev_tf_idf = tf_idf_vectorizer.transform(X_dev)
X_test_tf_idf = tf_idf_vectorizer.transform(X_test)

# Multi-task case
# X_train_tf_idf_mult = tf_idf_vectorizer.fit_transform(X_train_mult)

#tokenize and build vocabulary
#vectorizer_perturbation.fit(perturbation_data_text)
#encode the document
# X_dev_tf_idf_mult = tf_idf_vectorizer.transform(X_dev_mult)
# X_test_tf_idf_mult = tf_idf_vectorizer.transform(X_test_mult)


#Encoding categorical values into numerical values

Encoder_ = LabelEncoder()
y_train = Encoder_.fit_transform(y_train)
y_dev = Encoder_.fit_transform(y_dev)
y_test = Encoder_.fit_transform(y_test)

# Encoder = LabelEncoder()
# y_train_mult = Encoder.fit_transform(y_train_mult)
# y_dev_mult = Encoder.fit_transform(y_dev_mult)
# y_test_mult = Encoder.fit_transform(y_test_mult)



# #summarize
#print(f" Vocabulary of vectorizer on original data is {tf_idf_vectorizer.vocabulary_}")
#print(f" Vocabulary of vectorizer on pertubated data is {vectorizer_perturbation.vocabulary_}")
#print(vectorizer_perturbation.idf_)
print(f" Shape of X_train is  {X_train_tf_idf.shape}")
print(f" Shape of X_dev is  {X_dev_tf_idf.shape}")
print(f" Shape of X_test is  {X_test_tf_idf.shape}")
print("\n")
# print(f" Shape of X_train_mult is  {X_train_tf_idf_mult.shape}")
# print(f" Shape of X_dev_mult is  {X_dev_tf_idf_mult.shape}")
# print(f" Shape of X_test_mult is  {X_test_tf_idf_mult.shape}")

Encoder_.classes_

hate_to_code = dict(zip(Encoder_.classes_, Encoder_.transform(Encoder_.classes_)))
print(hate_to_code)

"""# <h2> *Baseline Models Evaluation* </h2>

## <h3> *Tf-idf - FastText - GloVE Building* </h3>
"""

start = time.time()
# # # Get UTF-8 format for text data
# #new_df.text.to_csv('text.txt', header=False, index=False)
# # Train our fasttext model
# ftt = fasttext.train_supervised(input = 'text.txt',
#                                   lr=1.0,
#                                   minn=2, maxn=5,
#                                   dim = 300,
#                                   epoch=30,
#                                   wordNgrams=3,
#                                   verbose=3)
# Tokenize text
tokenizer = Tokenizer()
new_df['tokenized'] = new_df.text.apply(lambda text: text.split())
# #  ################################### FASTTEXT GENSIM and GLoVE BUILDING ############################################
# # # Fasttext model initialization and building
fasttext_model= FastText(vector_size=300,alpha=0.025,window=5,
                         max_final_vocab = 20000,workers=5)
fasttext_model.build_vocab(new_df.tokenized)
fasttext_model.train(new_df.tokenized, total_examples=len(new_df.tokenized),
                     epochs=40)
fasttext_model.save("FastText_file_2_300.bin")

# We build 3 fastText models: one with max_final_vocab = 10k, another with max_final_vocab = 5k and the last one without this parameter.
# FastText model loading
# fast_text_model_ = FastText.load("/content/FastText_file_2_50.bin")
# fast_text_model_ = FastText.load("/content/FastText_file_2_100.bin")
# fast_text_model_ = FastText.load("/content/FastText_file_2_200.bin")
fast_text_model_ = FastText.load("/content/FastText_file_2_300.bin")
# # # fast_text_model_1 = FastText.load("FastText_file_0_.bin")
# # fast_text_model_2 = FastText.load("FastText_file_1_.bin")
# # GloVE building
# glove_model_ = load_embeddings('/content/drive/MyDrive/glove.6B.50d.txt')
# glove_model_ = load_embeddings('/content/drive/MyDrive/glove.6B.100d.txt')
# glove_model_ = load_embeddings('/content/drive/MyDrive/glove.6B.200d.txt')
glove_model_ = load_embeddings('/content/drive/MyDrive/glove.6B.300d.txt')

# # Vocabulary building
# We will use the same vocabulary for both FastText and Glove
# vocab = fast_text_model_.wv.key_to_index
# print(word_index)
# vocab_size_ = len(vocab) + 1
# print(vocab_size_)

end = round(time.time()-start,2)
print("This process took",end,"seconds.")

"""## <h4> *Binary task* </h4>"""

# # # Vectorize data
start = time.time()
# # param for vectorize_text function : text, emb_model, emb_dim, max_seq, average = False
X_train_ftt =  np.asarray([vectorize_text(text,fast_text_model_,300, 100, True) for text in X_train])
X_dev_ftt = np.asarray([vectorize_text(text,fast_text_model_,300, 100, True) for text in X_dev])
X_test_ftt = np.asarray([vectorize_text(text,fast_text_model_,300, 100, True) for text in X_test])

X_train_glove_ = np.asarray([vectorize_text(text,glove_model_,300, 100, True) for text in X_train])
X_dev_glove_ = np.asarray([vectorize_text(text,glove_model_,300, 100, True) for text in X_dev])
X_test_glove_ = np.asarray([vectorize_text(text,glove_model_,300,100 ,True) for text in X_test])

end = round(time.time()-start,2)
minutes = end // 60 # Get whole minutes
seconds_remaining = end % 60 # Get remaining seconds
print(f"This process took {int(minutes)} minutes, {seconds_remaining:.2f} seconds")

print(X_train.shape)
print(X_train_tf_idf.shape)
print(X_train_ftt.shape)
print(X_train_glove_.shape)
print('\n')
print(X_dev.shape)
print(X_dev_tf_idf.shape)
print(X_dev_ftt.shape)
print(X_dev_glove_.shape)
print('\n')
print(X_test.shape)
print(X_test_tf_idf.shape)
print(X_test_ftt.shape)
print(X_test_glove_.shape)

print(y_train.shape)
print(y_dev.shape)
print(y_test.shape)
print('\n')

"""## <h4> *Multi-task* </h4>"""

# # # Vectorize data    Multitask case
start = time.time()
# # param for vectorize_text function : text, emb_model, emb_dim, max_seq, average = False
X_train_ftt_mult =  np.asarray([vectorize_text(text,fast_text_model_,300, 100, True) for text in X_train_mult])
X_dev_ftt_mult = np.asarray([vectorize_text(text,fast_text_model_,300, 100, True) for text in X_dev_mult])
X_test_ftt_mult = np.asarray([vectorize_text(text,fast_text_model_,300, 100, True) for text in X_test_mult])

X_train_glove_mult = np.asarray([vectorize_text(text,glove_model_,300, 100, True) for text in X_train_mult])
X_dev_glove_mult = np.asarray([vectorize_text(text,glove_model_,300, 100, True) for text in X_dev_mult])
X_test_glove_mult = np.asarray([vectorize_text(text,glove_model_,300,100 ,True) for text in X_test_mult])

end = round(time.time()-start,2)
minutes = end // 60 # Get whole minutes
seconds_remaining = end % 60 # Get remaining seconds
print(f"This process took {int(minutes)} minutes, {seconds_remaining:.2f} seconds")

print(X_train_tf_idf_mult.shape)
print(X_train_ftt_mult.shape)
print(X_train_glove_mult.shape)
print('\n')
print(X_dev_tf_idf_mult.shape)
print(X_dev_ftt_mult.shape)
print(X_dev_glove_mult.shape)
print('\n')
print(X_test_tf_idf_mult.shape)
print(X_test_ftt_mult.shape)
print(X_test_glove_mult.shape)

# start = time.time()
# pca_model = PCA(n_components = 500).fit(X_train_ftt)
# print("Sum of variance ratios: ",sum(pca_model.explained_variance_ratio_))
# end = round(time.time()-start,2)
# minutes = end // 60 # Get whole minutes
# seconds_remaining = end % 60 # Get remaining seconds
# print(f"This process took {int(minutes)} minutes, {seconds_remaining:.2f} seconds")

# start = time.time()
# pca_model_glove = PCA(n_components = 3000).fit(X_train_glove_)
# print("Sum of variance ratios: ",sum(pca_model_glove.explained_variance_ratio_))
# end = round(time.time()-start,2)
# minutes = end // 60 # Get whole minutes
# seconds_remaining = end % 60 # Get remaining seconds
# print(f"This process took {int(minutes)} minutes, {seconds_remaining:.2f} seconds")

# # # PCA reduction

# start = time.time()
# # # param for vectorize_text function : text, emb_model, emb_dim, max_seq, average = False
# X_train_ftt_pca = pca_model.transform(X_train_ftt)
# X_dev_ftt_pca = pca_model.transform(X_dev_ftt)
# X_test_ftt_pca = pca_model.transform(X_test_ftt)

# X_train_glove_pca = pca_model_glove.transform(X_train_glove_)
# X_dev_glove_pca = pca_model_glove.transform(X_dev_glove_)
# X_test_glove_pca = pca_model_glove.transform(X_test_glove_)


# end = round(time.time()-start,2)
# minutes = end // 60 # Get whole minutes
# seconds_remaining = end % 60 # Get remaining seconds
# print(f"This process took {int(minutes)} minutes, {seconds_remaining:.2f} seconds")

# print(X_train.shape)
# print(X_train_tf_idf.shape)
# print(X_train_ftt_pca.shape)
# print(X_train_glove_pca.shape)
# print('\n')
# print(X_dev.shape)
# print(X_dev_tf_idf.shape)
# print(X_dev_ftt_pca.shape)
# print(X_dev_glove_pca.shape)
# print('\n')
# print(X_test.shape)
# print(X_test_tf_idf.shape)
# print(X_test_ftt_pca.shape)
# print(X_test_glove_pca.shape)

# import gensim
# def keep_words(word, count, min_count):
#     if word in word_list:
#         return gensim.utils.RULE_KEEP
#     else:
#         return gensim.utils.RULE_DEFAULT

# word_list = ['word1', 'word2', ...] # List of words to keep
# model.build_vocab(corpus, trim_rule=keep_words)

"""## <h3>*GridSearch CV Evaluation*</h3>

## <h4>*Binary task*</h4>
"""

sgd_clf = SGDClassifier(loss='hinge')

param_grid = {
    'alpha': [0.01, 0.001, 0.0001],
    'penalty': ['l1', 'l2'],
}

# Create a GridSearchCV object
grid_search = GridSearchCV(sgd_clf, param_grid, cv=10, scoring='accuracy', verbose = 3)

# Fit the GridSearchCV object to  data
grid_search.fit(X_train_tf_idf, y_train)

print(grid_search.best_params_)
# Get the best estimator
best_sgd_clf = grid_search.best_estimator_
#{'alpha': 0.0001, 'penalty': 'l2'}
y_pred = best_sgd_clf.predict(X_dev_tf_idf)
print(classification_report(y_dev, y_pred))

sgd_clf = SGDClassifier(loss='hinge')

param_grid = {
    'alpha': [0.01, 0.001, 0.0001],
    'penalty': ['l1', 'l2'],
}

# Create a GridSearchCV object
grid_search = GridSearchCV(sgd_clf, param_grid, cv=10, scoring='accuracy', verbose = 3)

# Fit the GridSearchCV object to  data
grid_search.fit(X_train_ftt, y_train)

print(grid_search.best_params_)
# Get the best estimator
best_sgd_clf = grid_search.best_estimator_
#{'alpha': 0.0001, 'penalty': 'l2'}
y_pred = best_sgd_clf.predict(X_dev_ftt)
print(classification_report(y_dev, y_pred))

sgd_clf = SGDClassifier(loss='hinge')

param_grid = {
    'alpha': [0.01, 0.001, 0.0001],
    'penalty': ['l1', 'l2'],
}

# Create a GridSearchCV object
grid_search = GridSearchCV(sgd_clf, param_grid, cv=10, scoring='accuracy', verbose = 3)

# Fit the GridSearchCV object to  data
grid_search.fit(X_train_glove_, y_train)

print(grid_search.best_params_)
# Get the best estimator
best_sgd_clf = grid_search.best_estimator_
#{'alpha': 0.0001, 'penalty': 'l2'}
y_pred = best_sgd_clf.predict(X_dev_glove_)
print(classification_report(y_dev, y_pred))

#SVM parameters
lsvc_model = LinearSVC(max_iter= 10000)
param_grid_lsvc = {'C': [0.1, 1, 10, 100]}

#GridSearch
grid_lsvc = GridSearchCV(lsvc_model,param_grid_lsvc, verbose = 3, cv = 10, scoring ='accuracy')
#fitting the model
grid_lsvc.fit(X_train_tf_idf,y_train)
#print best params
print(grid_lsvc.best_params_)
#Predictions
y_pred = grid_lsvc.best_estimator_.predict(X_dev_tf_idf)
# # print classification report
print(classification_report(y_dev, y_pred))
#{'C': 0.1}

# ##LOGISTIC REGRESSION

# # Create a pipeline
# # pipe = make_pipeline(LogisticRegression(max_iter= 10))
# log_reg = LogisticRegression(max_iter = 10000)
# # Define parameter grid
# param_grid = {'C': [0.01, 0.1, 1],
#               'penalty': ['l1', 'l2'],
#               'solver': ['liblinear','saga'],
#               }


# # Create GridSearchCV object
# grid_search = GridSearchCV(log_reg, param_grid, scoring='accuracy', cv=10, verbose = 3)

# # Fit on training data
# grid_search.fit(X_train_tf_idf, y_train)
# #log_reg.fit(X_train_tf_idf, y_train)
# print(grid_search.best_params_)
# # Evaluate on validation set
# y_val_pred = grid_search.best_estimator_.predict(X_dev_tf_idf)
# print(classification_report(y_dev, y_val_pred))
# #{'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}

# #XGBOOST
# xgb = XGBClassifier(learning_rate = 0.1,
#  n_estimators=100,
#  max_depth=5,
#  min_child_weight=1,
#  gamma=0,
#  subsample=0.8,
#  colsample_bytree=0.8,
#  objective= 'binary:logistic',
#  nthread=4,
#  scale_pos_weight=1,
#  seed=27)

# #Fit it on data
# xgb.fit(X_train_tf_idf,y_train)
# #Predict dev set
# y_pred_xgb = xgb.predict(X_dev_tf_idf)
# print("\nModel Report")
# print(classification_report(y_dev, y_pred_xgb))

# # param_test1 = {
# #  'alpha':[0, 0.001, 0.005, 0.01, 0.05]
# # }

# # gsearch1 = GridSearchCV(estimator = XGBClassifier(learning_rate = 0.1,
# # n_estimators=140,colsample_bytree =0.8, subsample =0.8,
# # gamma=0.2, max_depth = 9, min_child_weight = 1,
# # objective= 'binary:logistic', scale_pos_weight=1, seed=27),
# # param_grid = param_test1, scoring='accuracy',cv=5,verbose =3)
# # gsearch1.fit(X_train_tf_idf,y_train)
# # # print(gsearch1.grid_scores_)
# # print(gsearch1.best_score_)
# # print(gsearch1.best_params_)
# #XGBOOST
# xgb = XGBClassifier(learning_rate =0.1,
#  n_estimators=100,
#  max_depth=9,
#  min_child_weight=1,
#  gamma=0,
#  subsample=0.8,
#  colsample_bytree=0.8,
#  objective= 'binary:logistic',
#  scale_pos_weight=1,
#  seed=27)

# #Fit it on data
# xgb.fit(X_train_tf_idf,y_train)
# #Predict dev set
# y_pred_xgb = xgb.predict(X_dev_tf_idf)
# print("\nModel Report")
# print(classification_report(y_dev, y_pred_xgb))

"""## <h3>*Multi task*</h3>"""

from sklearn.metrics import make_scorer, average_precision_score
pr_auc_scorer = make_scorer(average_precision_score, needs_proba=True)
sgd_clf = SGDClassifier(loss='hinge')

param_grid = {
    'alpha': [0.01, 0.001, 0.0001],
    'penalty': ['l1', 'l2'],
}

# Create a GridSearchCV object
grid_search = GridSearchCV(sgd_clf, param_grid, cv=10, scoring='pr_auc_scorer', verbose = 3)

# Fit the GridSearchCV object to  data
grid_search.fit(X_train_tf_idf_mult, y_train_mult)

print(grid_search.best_params_)
# Get the best estimator
best_sgd_clf = grid_search.best_estimator_
#{'alpha': 0.0001, 'penalty': 'l2'}
y_pred = best_sgd_clf.predict(X_dev_tf_idf_mult)
print(classification_report(y_dev_mult, y_pred))

sgd_clf = SGDClassifier(loss='hinge')

param_grid = {
    'alpha': [0.01, 0.001, 0.0001],
    'penalty': ['l1', 'l2'],
}

# Create a GridSearchCV object
grid_search = GridSearchCV(sgd_clf, param_grid, cv=10, scoring='accuracy', verbose = 3)

# Fit the GridSearchCV object to  data
grid_search.fit(X_train_ftt_mult, y_train_mult)

print(grid_search.best_params_)
# Get the best estimator
best_sgd_clf = grid_search.best_estimator_
#{'alpha': 0.0001, 'penalty': 'l2'}
y_pred = best_sgd_clf.predict(X_dev_ftt_mult)
print(classification_report(y_dev_mult, y_pred))

# #SVM parameters
# lsvc_model = LinearSVC(max_iter= 10000)
# param_grid_lsvc = {'C': [0.1, 1, 10, 100]}

# #GridSearch
# grid_lsvc = GridSearchCV(lsvc_model,param_grid_lsvc, verbose = 3, cv = 10, scoring ='accuracy')
# #fitting the model
# grid_lsvc.fit(X_train_tf_idf,y_train)
# #print best params
# print(grid_lsvc.best_params_)
# #Predictions
# y_pred = grid_lsvc.best_estimator_.predict(X_dev_tf_idf)
# # # print classification report
# print(classification_report(y_dev, y_pred))
# #{'C': 0.1}

# ##LOGISTIC REGRESSION

# # Create a pipeline
# # pipe = make_pipeline(LogisticRegression(max_iter= 10))
# log_reg = LogisticRegression(max_iter = 10000)
# # Define parameter grid
# param_grid = {'C': [0.01, 0.1, 1],
#               'penalty': ['l1', 'l2'],
#               'solver': ['liblinear','saga'],
#               }


# # Create GridSearchCV object
# grid_search = GridSearchCV(log_reg, param_grid, scoring='accuracy', cv=10, verbose = 3)

# # Fit on training data
# grid_search.fit(X_train_tf_idf, y_train)
# #log_reg.fit(X_train_tf_idf, y_train)
# print(grid_search.best_params_)
# # Evaluate on validation set
# y_val_pred = grid_search.best_estimator_.predict(X_dev_tf_idf)
# print(classification_report(y_dev, y_val_pred))
# #{'C': 1, 'penalty': 'l2', 'solver': 'liblinear'}

# #XGBOOST
# xgb = XGBClassifier(learning_rate = 0.1,
#  n_estimators=100,
#  max_depth=5,
#  min_child_weight=1,
#  gamma=0,
#  subsample=0.8,
#  colsample_bytree=0.8,
#  objective= 'binary:logistic',
#  nthread=4,
#  scale_pos_weight=1,
#  seed=27)

# #Fit it on data
# xgb.fit(X_train_tf_idf,y_train)
# #Predict dev set
# y_pred_xgb = xgb.predict(X_dev_tf_idf)
# print("\nModel Report")
# print(classification_report(y_dev, y_pred_xgb))

# # param_test1 = {
# #  'alpha':[0, 0.001, 0.005, 0.01, 0.05]
# # }

# # gsearch1 = GridSearchCV(estimator = XGBClassifier(learning_rate = 0.1,
# # n_estimators=140,colsample_bytree =0.8, subsample =0.8,
# # gamma=0.2, max_depth = 9, min_child_weight = 1,
# # objective= 'binary:logistic', scale_pos_weight=1, seed=27),
# # param_grid = param_test1, scoring='accuracy',cv=5,verbose =3)
# # gsearch1.fit(X_train_tf_idf,y_train)
# # # print(gsearch1.grid_scores_)
# # print(gsearch1.best_score_)
# # print(gsearch1.best_params_)
# #XGBOOST
# xgb = XGBClassifier(learning_rate =0.1,
#  n_estimators=100,
#  max_depth=9,
#  min_child_weight=1,
#  gamma=0,
#  subsample=0.8,
#  colsample_bytree=0.8,
#  objective= 'binary:logistic',
#  scale_pos_weight=1,
#  seed=27)

# #Fit it on data
# xgb.fit(X_train_tf_idf,y_train)
# #Predict dev set
# y_pred_xgb = xgb.predict(X_dev_tf_idf)
# print("\nModel Report")
# print(classification_report(y_dev, y_pred_xgb))

"""## <h3> *Dev Evaluation* </h3>"""

#All models (after being tuned by GridSearch Cross Validation)
# knn_model = KNeighborsClassifier(n_neighbors=5)
# rfc_model = RandomForestClassifier()
sgd_clf = SGDClassifier(loss='hinge',alpha = 0.0001, penalty = 'l2')
lsvc_model = LinearSVC(C = 0.1, max_iter = 10000,dual=True)
log_reg = LogisticRegression(C = 1, penalty = 'l2', solver = 'liblinear', max_iter = 10000)
xgb = XGBClassifier(learning_rate = 0.1, n_estimators=100, max_depth=5, min_child_weight=1, gamma=0, subsample=0.8, colsample_bytree=0.8,
                    objective= 'binary:logistic', nthread=4, scale_pos_weight=1, seed=27)
# 'Linear SVM classifier': lsvc_model

models = {'SGD classifier':sgd_clf, 'Linear SVM classifier': lsvc_model, 'Logistic Regression classifier': log_reg, 'XGBOOST classifier': xgb}

# pipeline = Pipeline([
#     ('vect', TfidfVectorizer()),
#     ('xgb', xgb )
# ])

# # Define the parameter grid for TfidfVectorizer
# param_grid = {
#     'vect__max_df': (0.5, 0.75, 1.0),
#     'vect__max_features': (None, 5000, 10000),
#     'vect__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams
#     'vect__use_idf': (True, False),
#     'vect__norm': ('l1', 'l2')
# }

# # Create the grid search object
# grid_search = GridSearchCV(pipeline, param_grid, cv=5, verbose= 3)

# # Fit the grid search object to the data
# grid_search.fit(X_train, y_train)

# # Print the best parameters
# print("Best parameters: ", grid_search.best_params_)

# # Get the best estimator
# best_model = grid_search.best_estimator_

# log_reg.fit(X_train_ftt_gensim,y_train)
# y_pred_temp = log_reg.predict(y_dev)
# print(classification_report(y_dev,y_pred_temp))

"""## <h3>*TF-idf- FastText- GloVE Evaluation - All*</h3>"""

#TF-IDF Case
start = time.time()
best_model_tf_idf = evaluate_model_ml_cv(models, X_train_tf_idf, X_dev_tf_idf, X_test_tf_idf, y_train, y_dev, y_test)
end = round(time.time()-start,2)
print("This process took",end,"seconds.")

# #ROC Curve XGBOOST
draw_roc_curve(best_model_tf_idf, X_test_tf_idf, y_test)

# # FastText case
start = time.time()
best_model_fasttext = evaluate_model_ml_cv(models, X_train_ftt, X_dev_ftt, X_test_ftt, y_train, y_dev, y_test)
end = round(time.time()-start,2)
print("This process took",end,"seconds.")

draw_roc_curve(best_model_fasttext, X_test_ftt, y_test)

# #Glove case
start = time.time()
best_model_glove = evaluate_model_ml_cv(models, X_train_glove_, X_dev_glove_, X_test_glove_, y_train, y_dev, y_test)
end = round(time.time()-start,2)
print("This process took",end,"seconds.")

draw_roc_curve(best_model_glove, X_test_glove_, y_test)

"""## <h3>*TF-idf- FastText- GloVE Evaluation - Perturbated-Original*</h3>"""

#TF-IDF Case
start = time.time()
best_model_tf_idf = evaluate_model_ml_cv(models, X_train_tf_idf, X_dev_tf_idf, X_test_tf_idf, y_train, y_dev, y_test)
end = round(time.time()-start,2)
print("This process took",end,"seconds.")

# #ROC Curve
draw_roc_curve(best_model_tf_idf, X_test_tf_idf, y_test)

# # FastText case
start = time.time()
best_model_fasttext = evaluate_model_ml_cv(models, X_train_ftt, X_dev_ftt, X_test_ftt, y_train, y_dev, y_test)
end = round(time.time()-start,2)
print("This process took",end,"seconds.")

draw_roc_curve(best_model_fasttext, X_test_ftt, y_test)

# #Glove case
start = time.time()
best_model_glove = evaluate_model_ml_cv(models, X_train_glove_, X_dev_glove_, X_test_glove_, y_train, y_dev, y_test)
end = round(time.time()-start,2)
print("This process took",end,"seconds.")

draw_roc_curve(best_model_glove, X_test_glove_, y_test)

"""## <h3>*TF-idf- FastText- GloVE Evaluation - Original-perturbated*</h3>"""

#TF-IDF Case
start = time.time()
best_model_tf_idf = evaluate_model_ml_cv(models, X_train_tf_idf, X_dev_tf_idf, X_test_tf_idf, y_train, y_dev, y_test)
end = round(time.time()-start,2)
print("This process took",end,"seconds.")

# #ROC Curve
draw_roc_curve(best_model_tf_idf, X_test_tf_idf, y_test)

# # FastText case
start = time.time()
best_model_fasttext = evaluate_model_ml_cv(models, X_train_ftt, X_dev_ftt, X_test_ftt, y_train, y_dev, y_test)
end = round(time.time()-start,2)
print("This process took",end,"seconds.")

draw_roc_curve(best_model_fasttext, X_test_ftt, y_test)

# #Glove case
start = time.time()
best_model_glove = evaluate_model_ml_cv(models, X_train_glove_, X_dev_glove_, X_test_glove_, y_train, y_dev, y_test)
end = round(time.time()-start,2)
print("This process took",end,"seconds.")

draw_roc_curve(best_model_glove, X_test_glove_, y_test)

"""# <h2> *DL models* </h2>

##<h3>*Data preparation*</h3>
"""

# tokenizer = AutoTokenizer.from_pretrained("cardiffnlp/twitter-roberta-base-emotion")
# tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')
#tokenizer = AutoTokenizer.from_pretrained("cardiffnlp/twitter-roberta-base-emotion")
tokenizer = AutoTokenizer.from_pretrained("ydshieh/bert-base-uncased-yelp-polarity")
# tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)
X_train_roberta = roberta_encode(X_train, tokenizer)
X_dev_roberta = roberta_encode(X_dev, tokenizer)
X_test_roberta = roberta_encode(X_test, tokenizer)

# X_train_facebook_bart = facebook_bart_encode(X_train, tokenizer)
# X_dev_facebook_bart = facebook_bart_encode(X_dev, tokenizer)
# X_test_facebook_bart = facebook_bart_encode(X_test, tokenizer)

text_vector = TextVectorization(max_tokens = 20000, output_sequence_length = 100)
text_vector.adapt(new_df.text) # Analyzes the data and builds the vocabulary.
# Encoding text data into integer sequences
MAX_LENGTH = 100
vocab = text_vector.get_vocabulary()
vocabulary_size = text_vector.vocabulary_size()
print(f"Vocabulary is comprised of : {vocab}")
print(f" Size of vocabulary of vectorizer on original data is {text_vector.vocabulary_size()}")
# Encode text data
X_train_dl = encode_text(text = X_train,text_vectorizer = text_vector,
                         max_length = MAX_LENGTH)
X_dev_dl = encode_text(text = X_dev,text_vectorizer = text_vector,
                         max_length = MAX_LENGTH)
X_test_dl =  encode_text(text = X_test,text_vectorizer = text_vector,
                         max_length = MAX_LENGTH)
print(f" X_train has a shape of : {X_train_dl.shape}")
print(f" X_dev has a shape of : {X_dev_dl.shape}")
print(f" X_test has a shape of : {X_test_dl.shape}")

fastText_embedding_matrix = build_embedding_matrix(vocab,fast_text_model_,300,True)
embedding_matrix_glove = build_embedding_matrix(vocab,glove_model_,300,True)

"""# <h3> *DL work* </h3>

##<h4> *Attention mechanism implementations* </h4>
"""

# Add attention layer to the deep learning network
class Attention_(Layer):
    def __init__(self,**kwargs):
        super(Attention_,self).__init__(**kwargs)

    def build(self,input_shape):
        # Define the shape of the weights and bias in this layer
        # The layer has just 1 lonely neuron
        self.W=self.add_weight(name='attention_weight', shape=(input_shape[-1],1),
                               initializer='random_normal', trainable=True)
        self.b=self.add_weight(name='attention_bias', shape=(input_shape[1],1),
                               initializer='zeros', trainable=True)
        self.features_dim = input_shape[-1]
        super(Attention_, self).build(input_shape)

    def call(self,x):
        features_dim = self.features_dim
        # x is the input tensor
        # Alignment scores. Pass them through tanh function
        #x =  tf.reshape(x,[-1])
        # eij = tf.tensordot(tf.reshape(x, (-1, features_dim)),
        #                      tf.reshape(self.W, (features_dim, 1)), 1)
        x = tf.reshape(x, (-1,features_dim))
        W = tf.reshape(self.W,(features_dim, 1))

        e = tf.tanh(tf.tensordot(x, W, 1)+self.b)
        # We need to squeeze the (max_seq_length * 1) attention vector into a 1-D array
        # before passing to softmax function
        e = tf.keras.ops.squeeze(e, axis=-1)
        # e = Flatten()(e)
        # Compute the weights
        # Softmax squashes these into values in the range between 0, and 1 whose sum is 1
        alpha = tf.keras.activations.softmax(e)
        # Reshape to tensorFlow format
        # We need to expand back the attention weights from (max_seq_length) to (max_seq_length * 1)
        alpha = tf.expand_dims(alpha, axis=-1)
        # Compute the context vector
        # We multiply each attention weight by the respective word and sum up
        context = x * alpha
        context = tf.reduce_sum(context, axis=1)
        return context

    def compute_output_shape(self, input_shape):
        return (input_shape[0], input_shape[-1])

from keras import backend as K
import keras
class AttentionLayer(Layer):

    def __init__(self, step_dim,
                 W_regularizer=None, b_regularizer=None,
                 W_constraint=None, b_constraint=None,
                 bias=True, **kwargs):

        self.supports_masking = True
        self.init = keras.initializers.get('glorot_uniform')

        self.W_regularizer = keras.regularizers.get(W_regularizer)
        self.b_regularizer = keras.regularizers.get(b_regularizer)

        self.W_constraint = keras.constraints.get(W_constraint)
        self.b_constraint = keras.constraints.get(b_constraint)

        self.bias = bias
        self.step_dim = step_dim
        self.features_dim = 0
        super(AttentionLayer, self).__init__(**kwargs)


    def build(self, input_shape):
        # assert len(input_shape) == 3

        self.W = self.add_weight(shape=(input_shape[-1],),
                                 initializer=self.init,
                                 name='{}_W'.format(self.name),
                                 regularizer=self.W_regularizer,
                                 constraint=self.W_constraint)
        self.features_dim = input_shape[-1]

        if self.bias:
            self.b = self.add_weight(shape=(input_shape[1],),
                                     initializer='zero',
                                     name='{}_b'.format(self.name),
                                     regularizer=self.b_regularizer,
                                     constraint=self.b_constraint)
        else:
            self.b = None

        self.built = True


    def compute_mask(self, input, input_mask=None):
        # do not pass the mask to the next layers
        return None


    def call(self, x, mask=None):
        # TF backend doesn't support it
        # eij = K.dot(x, self.W)
        # features_dim = self.W.shape[0]
        # step_dim = x._keras_shape[1]

        features_dim = self.features_dim
        step_dim = self.step_dim
        eij = tf.reshape(tf.tensordot(tf.reshape(x, (-1, features_dim)),
                             tf.reshape(self.W, (features_dim, 1)), 1),
                        (-1, step_dim))
        # eij = tf.tensordot(tf.reshape(x, (-1, features_dim)),
        #                      tf.reshape(self.W, (features_dim, 1)), 1)

        if self.bias:
            eij += self.b

        eij = tf.tanh(eij)

        a = tf.exp(eij)

        # apply mask after the exp. will be re-normalized next
        if mask is not None:
            # Cast the mask to floatX to avoid float64 upcasting in theano
            a *= tf.cast(mask, K.floatx())

        # in some cases especially in the early stages of training the sum may be almost zero
        a /= tf.cast(tf.reduce_sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())
        a = tf.expand_dims(a, axis =-1)
        weighted_input = x * a

        return tf.reduce_sum(weighted_input, axis=1)


    def compute_output_shape(self, input_shape):
        return input_shape[0],  self.features_dim


    def get_config(self):
        config = {'step_dim': self.step_dim}
        base_config = super(AttentionLayer, self).get_config()
        return dict(list(base_config.items()) + list(config.items()))

"""##<h4> *CNN implementations* </h4>"""

from os import name
MAX_SEQUENCE_LENGTH = 100
vocabulary_size = text_vector.vocabulary_size()
# After GridSearch
# # CNN
def build_model_cnn(filters, kernel_size, dropout_rate,
                    dense_units, emb_dim):
    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32)
    embedding_sequences = Embedding(input_dim = vocabulary_size  + 1,
                          output_dim = emb_dim)(sequence_input)

    spatial_dropout = SpatialDropout1D(dropout_rate)(embedding_sequences)

    conv_1d = Conv1D(filters, kernel_size, activation='relu')(spatial_dropout)
    conv_2d = Conv1D(int(filters/2), kernel_size, activation='relu')(spatial_dropout)
    conv_3d = Conv1D(int(filters/4), kernel_size, activation='relu')(spatial_dropout)
    conv_4d = Conv1D(int(filters/8), kernel_size, activation='relu')(spatial_dropout)

    max_pooling_conv_1d = GlobalMaxPooling1D()(conv_1d)
    max_pooling_conv_2d = GlobalMaxPooling1D()(conv_2d)
    max_pooling_conv_3d = GlobalMaxPooling1D()(conv_3d)
    max_pooling_conv_4d = GlobalMaxPooling1D()(conv_4d)
    concatenated = concatenate([max_pooling_conv_1d, max_pooling_conv_2d,
                               max_pooling_conv_3d, max_pooling_conv_4d])
    # flatten = Flatten()(max_pooling_conv_1d)
    # flatten = Flatten()(concatenated)
    # attention = AttentionLayer(MAX_SEQUENCE_LENGTH)(concatenated)
    dense = Dense(dense_units, activation='relu')(concatenated)
    dropout = Dropout(dropout_rate)(dense)
    output = Dense(1, activation='sigmoid')(dropout)

    model = Model(inputs=sequence_input, outputs=output,name = 'model_cnn')
    # compile network
    adagrad = tf.keras.optimizers.Adagrad(learning_rate=0.00001)
    model.compile(loss='binary_crossentropy', optimizer= adagrad,
                  metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])
    # summarize defined model
    model.summary()
    plot_model(model, to_file='model_cnn.png', show_shapes=True)
    return model

# CNN with FastText word-embedding
def build_model_cnn_fastText(filters, kernel_size, dropout_rate,
                    dense_units, emb_dim):
    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32)
    embedding_sequences = Embedding(input_dim = vocabulary_size  + 1,
                          output_dim = emb_dim,embeddings_initializer =
                        Constant(fastText_embedding_matrix),
                        trainable = False)(sequence_input)

    spatial_dropout = SpatialDropout1D(dropout_rate)(embedding_sequences)

    conv_1d = Conv1D(filters, kernel_size, activation='relu')(spatial_dropout)
    conv_2d = Conv1D(int(filters/2), kernel_size, activation='relu')(spatial_dropout)
    conv_3d = Conv1D(int(filters/4), kernel_size, activation='relu')(spatial_dropout)
    conv_4d = Conv1D(int(filters/8), kernel_size, activation='relu')(spatial_dropout)

    max_pooling_conv_1d = GlobalMaxPooling1D()(conv_1d)
    max_pooling_conv_2d = GlobalMaxPooling1D()(conv_2d)
    max_pooling_conv_3d = GlobalMaxPooling1D()(conv_3d)
    max_pooling_conv_4d = GlobalMaxPooling1D()(conv_4d)
    concatenated = concatenate([max_pooling_conv_1d, max_pooling_conv_2d,
                                max_pooling_conv_3d, max_pooling_conv_4d])
    #flatten = Flatten()(max_pooling_conv_1d)
    #attention = AttentionLayer(MAX_SEQUENCE_LENGTH)(conv_1d)
    dense = Dense(dense_units, activation='relu')(concatenated)
    dropout = Dropout(dropout_rate)(dense)
    output = Dense(1, activation='sigmoid')(dropout)

    model = Model(inputs=sequence_input, outputs=output, name = 'model_cnn_fastText')
    # compile network
    adagrad = tf.keras.optimizers.Adagrad(learning_rate=0.00001)
    model.compile(loss='binary_crossentropy', optimizer= adagrad,
                  metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])
    # summarize defined model
    model.summary()
    plot_model(model, to_file='model_cnn_fastText300_matrix.png', show_shapes=True)
    return model

def build_model_cnn_glove(filters, kernel_size, dropout_rate,
                    dense_units, emb_dim):
    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32)
    embedding_sequences = Embedding(input_dim = vocabulary_size  + 1,
                          output_dim = emb_dim,embeddings_initializer =
                        Constant(embedding_matrix_glove),
                        trainable = False)(sequence_input)

    spatial_dropout = SpatialDropout1D(dropout_rate)(embedding_sequences)

    conv_1d = Conv1D(filters, kernel_size, activation='relu')(spatial_dropout)
    conv_2d = Conv1D(int(filters/2), kernel_size, activation='relu')(spatial_dropout)
    conv_3d = Conv1D(int(filters/4), kernel_size, activation='relu')(spatial_dropout)
    conv_4d = Conv1D(int(filters/8), kernel_size, activation='relu')(spatial_dropout)

    max_pooling_conv_1d = GlobalMaxPooling1D()(conv_1d)
    max_pooling_conv_2d = GlobalMaxPooling1D()(conv_2d)
    max_pooling_conv_3d = GlobalMaxPooling1D()(conv_3d)
    max_pooling_conv_4d = GlobalMaxPooling1D()(conv_4d)
    concatenated = concatenate([max_pooling_conv_1d, max_pooling_conv_2d,
                                max_pooling_conv_3d, max_pooling_conv_4d])
    # flatten = Flatten()(max_pooling_conv_1d)
    #attention = Attention()(conv_1d)
    dense = Dense(dense_units, activation='relu')(concatenated)
    dropout = Dropout(dropout_rate)(dense)
    output = Dense(1, activation='sigmoid')(dropout)

    model = Model(inputs=sequence_input, outputs=output, name = 'model_cnn_glove')
    # compile network
    adagrad = tf.keras.optimizers.Adagrad(learning_rate=0.00001)
    model.compile(loss='binary_crossentropy', optimizer= adagrad,
                  metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])
    # summarize defined model
    model.summary()
    plot_model(model, to_file='model_cnn_glove300_matrix.png', show_shapes=True)
    return model

"""##<h4> *LSTM implementations* </h4>"""

MAX_SEQUENCE_LENGTH = 100
vocabulary_size = text_vector.vocabulary_size()
# LSTM
def build_model_lstm(lstm_units, k_regularizer, dense_units,
                     dropout_rate, recurrent_dropout, emb_dim):
    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32)
    embeddings = Embedding(input_dim = vocabulary_size + 1, output_dim = emb_dim)(sequence_input)

    spatial_dropout = (SpatialDropout1D(dropout_rate))(embeddings)
    lstm_1 = LSTM(units = lstm_units, kernel_regularizer = l1(k_regularizer),
                   recurrent_dropout = recurrent_dropout, return_sequences = True)(spatial_dropout)
    lstm_2 = LSTM(units = lstm_units, kernel_regularizer = l1(k_regularizer),
                   recurrent_dropout = recurrent_dropout, return_sequences = True)(spatial_dropout)
    concatenated_0 = concatenate([lstm_1, lstm_2])
    max_pooling = GlobalMaxPooling1D()(concatenated_0)
    attention = AttentionLayer(MAX_SEQUENCE_LENGTH)(concatenated_0)
    concatenated = concatenate([max_pooling, attention])
    # model.add(GlobalMaxPooling1D()) # for dimensionality reduction
    dense = Dense(dense_units, activation='relu')(concatenated)
    dropout = Dropout(dropout_rate)(dense)
    # Define the ouput layer
    output = Dense(1, activation='sigmoid')(dropout)
    model = Model(inputs=sequence_input, outputs=output, name = 'model_lstm')
    # compile network
    adam =  Adam(learning_rate = 0.001)
    model.compile(loss ='binary_crossentropy', optimizer = adam,
                  metrics =['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])
    model.summary()
    plot_model(model, to_file = 'model_lstm.png', show_shapes = True)
    return model

# LSTM with FastText matrix
def build_model_lstm_fastText(lstm_units, dense_units, k_regularizer,
                         dropout_rate, recurrent_dropout, emb_dim):

    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32)
    embeddings = Embedding(input_dim = vocabulary_size + 1, output_dim = emb_dim,
                           embeddings_initializer = Constant(fastText_embedding_matrix), trainable = False)(sequence_input)
    spatial_dropout = (SpatialDropout1D(dropout_rate))(embeddings)
    lstm_1 = LSTM(units = lstm_units, kernel_regularizer = l1(k_regularizer),
                   recurrent_dropout = recurrent_dropout, return_sequences = True)(spatial_dropout)
    max_pooling = GlobalMaxPooling1D()(lstm_1)
    attention = AttentionLayer(MAX_SEQUENCE_LENGTH)(lstm_1)
    concatenated = concatenate([max_pooling, attention])
    # model.add(GlobalMaxPooling1D()) # for dimensionality reduction
    dense = Dense(dense_units, activation='relu')(concatenated)
    dropout = Dropout(dropout_rate)(dense)
    # Define the ouput layer
    output = Dense(1, activation='sigmoid')(dropout)
    model = Model(inputs=sequence_input, outputs=output, name = 'model_lstm_fastText')
    # compile network
    adam =  Adam(learning_rate = 0.001)
    model.compile(loss ='binary_crossentropy', optimizer = adam,
                  metrics =['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])
    model.summary()
    plot_model(model, to_file = 'model_lstm_fastText300_matrix.png',
               show_shapes = True)

    return model

# LSTM with GlovE matrix
def build_model_lstm_glove(lstm_units, dense_units, k_regularizer,
                         dropout_rate, recurrent_dropout, emb_dim):
    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32)
    embeddings = Embedding(input_dim = vocabulary_size + 1, output_dim = emb_dim,
                           embeddings_initializer = Constant(embedding_matrix_glove), trainable = False)(sequence_input)
    spatial_dropout = (SpatialDropout1D(dropout_rate))(embeddings)
    lstm_1 = LSTM(units = lstm_units, kernel_regularizer = l1(k_regularizer),
                   recurrent_dropout = recurrent_dropout, return_sequences = True)(spatial_dropout)
    max_pooling = GlobalMaxPooling1D()(lstm_1)
    attention = AttentionLayer(MAX_SEQUENCE_LENGTH)(lstm_1)
    concatenated = concatenate([max_pooling, attention])
    # model.add(GlobalMaxPooling1D()) # for dimensionality reduction
    dense = Dense(dense_units, activation='relu')(concatenated)
    dropout = Dropout(dropout_rate)(dense)
    # Define the ouput layer
    output = Dense(1, activation='sigmoid')(dropout)
    model = Model(inputs=sequence_input, outputs=output, name = 'model_lstm_glove')

    # compile network
    adam =  Adam(learning_rate = 0.001)
    model.compile(loss ='binary_crossentropy', optimizer = adam,
                  metrics =['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])
    model.summary()
    plot_model(model, to_file = 'model_lstm_glove300_matrix.png',
               show_shapes = True)

    return model

"""##<h4> *GRU implementations* </h4>"""

MAX_SEQUENCE_LENGTH = 100
vocabulary_size = text_vector.vocabulary_size()
# GRU
def build_model_gru(gru_units, k_regularizer, learning_rate,
                          dropout_rate, recurrent_dropout, emb_dim):

    sequence_input = (Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32))
    # model.add(text_vectorizer)
    embeddings = Embedding(input_dim = vocabulary_size + 1, output_dim = emb_dim)(sequence_input)
    spatial_dropout = (SpatialDropout1D(dropout_rate))(embeddings)
    gru = GRU(units = gru_units, kernel_regularizer = l1(k_regularizer),
                   recurrent_dropout = recurrent_dropout, return_sequences = True)(spatial_dropout)

    #model.add(GRU(gru_units))
    max_pooling = GlobalMaxPooling1D()(gru)
    #model.add(GRU(gru_units))
    #model.add(SpatialDropout1D(dropout_rate))
    attention = AttentionLayer(MAX_SEQUENCE_LENGTH)(gru)
    concatenated = concatenate([max_pooling, attention])
    # model.add(GlobalMaxPooling1D()) # for dimensionality reduction
    dense = Dense(gru_units, activation='relu')(concatenated)
    # dropout = Dropout(dropout_rate)(dense)
    output = Dense(1, activation='sigmoid')(dense)
    model = Model(inputs=sequence_input, outputs=output, name = 'model_gru')

    # compile network
    adam = Adam(learning_rate = learning_rate)
    model.compile(loss='binary_crossentropy', optimizer= adam,
                  metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])
    # summarize defined model
    model.summary()
    plot_model(model, to_file='model_gru.png', show_shapes=True)
    return model

# GRU with FastText matrix
def build_model_gru_fastText(gru_units, k_regularizer, learning_rate,
                          dropout_rate, recurrent_dropout, emb_dim):
    sequence_input = (Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32))

    embeddings = Embedding(input_dim = vocabulary_size + 1, output_dim = emb_dim,
                           embeddings_initializer= Constant(fastText_embedding_matrix),
                           trainable  = False)(sequence_input)
    spatial_dropout = (SpatialDropout1D(dropout_rate))(embeddings)
    # Adding 3 GRU layers
    gru_0 = GRU(units = gru_units, kernel_regularizer = l1(k_regularizer),
                   recurrent_dropout = recurrent_dropout, return_sequences = True)(spatial_dropout)
    # gru_1 = GRU(units = gru_units, kernel_regularizer = l1(k_regularizer),
    #                recurrent_dropout = recurrent_dropout, return_sequences = True)(spatial_dropout)
    # gru_2 = GRU(units = gru_units, kernel_regularizer = l1(k_regularizer),
    #                recurrent_dropout = recurrent_dropout, return_sequences = True)(spatial_dropout)
    # compact_gru = concatenate([gru_0, gru_1, gru_2])

    max_pooling = GlobalMaxPooling1D()(gru_0)
    attention = AttentionLayer(MAX_SEQUENCE_LENGTH)(gru_0)

    concatenated = concatenate([max_pooling, attention])

    dense = Dense(gru_units, activation='relu')(concatenated)
    # dropout = Dropout(dropout_rate)(dense)
    output = Dense(1, activation='sigmoid')(dense)
    model = Model(inputs=sequence_input, outputs=output, name = 'model_gru_fastText')
    # compile network
    adam = Adam(learning_rate = learning_rate)
    model.compile(loss='binary_crossentropy', optimizer= adam,
                  metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])
    # summarize defined model
    model.summary()
    plot_model(model, to_file='model_gru_fastText300_matrix.png',
               show_shapes=True)
    return model

# GRU with GloVE matrix
def build_model_gru_glove(gru_units, k_regularizer,learning_rate,
                          dropout_rate, recurrent_dropout, emb_dim):
    sequence_input = (Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32))
    # model.add(text_vectorizer)
    embeddings = Embedding(input_dim = vocabulary_size + 1, output_dim = emb_dim,
                           embeddings_initializer= Constant(embedding_matrix_glove),
                           trainable  = False)(sequence_input)
    spatial_dropout = (SpatialDropout1D(dropout_rate))(embeddings)
    # Adding 3 GRU layers
    gru_0 = GRU(units = gru_units, kernel_regularizer = l1(k_regularizer),
                   recurrent_dropout = recurrent_dropout, return_sequences = True)(spatial_dropout)
    # gru_1 = GRU(units = gru_units, kernel_regularizer = l1(k_regularizer),
    #                recurrent_dropout = recurrent_dropout, return_sequences = True)(spatial_dropout)
    # gru_2 = GRU(units = gru_units, kernel_regularizer = l1(k_regularizer),
    #                recurrent_dropout = recurrent_dropout, return_sequences = True)(spatial_dropout)
    # compact_gru = concatenate([gru_0, gru_1, gru_2])

    max_pooling = GlobalMaxPooling1D()(gru_0)
    attention = AttentionLayer(MAX_SEQUENCE_LENGTH)(gru_0)

    concatenated = concatenate([max_pooling, attention])

    dense = Dense(256, activation='relu')(concatenated)
    # dropout = Dropout(dropout_rate)(dense)
    output = Dense(1, activation='sigmoid')(dense)
    model = Model(inputs=sequence_input, outputs=output, name = 'model_gru_glove')

    # compile network
    adam = Adam(learning_rate = learning_rate)
    model.compile(loss='binary_crossentropy', optimizer= adam,
                  metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])
    # summarize defined model
    model.summary()
    plot_model(model, to_file='model_gru_glove300_matrix.png', show_shapes=True)
    return model

"""##<h4> *BiGRU implementations* </h4>"""

MAX_SEQUENCE_LENGTH = 100
vocabulary_size = text_vector.vocabulary_size()
# BIGRU
def build_model_bi_gru(gru_units, k_regularizer,learning_rate,
                          dropout_rate, recurrent_dropout, emb_dim):
    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32)
    embedding_sequences = Embedding(input_dim = vocabulary_size  + 1,
                          output_dim = emb_dim)(sequence_input)
    spatial_dropout = (SpatialDropout1D(dropout_rate))(embedding_sequences)
    bi_gru = Bidirectional(GRU(units = gru_units, kernel_regularizer = l1(k_regularizer),
                   recurrent_dropout = recurrent_dropout, return_sequences = True))(spatial_dropout)
    max_pooling = GlobalMaxPooling1D()(bi_gru)
    attention = AttentionLayer(MAX_SEQUENCE_LENGTH)(bi_gru)
    concatenated = concatenate([max_pooling, attention])

    dense = Dense(gru_units, activation='relu')(concatenated)
    dropout = Dropout(dropout_rate)(dense)
    output = Dense(1, activation='sigmoid')(dropout)

    model = Model(inputs=sequence_input, outputs=output, name = 'model_bigru')
    # compile network
    adam = Adam(learning_rate = learning_rate)
    model.compile(loss='binary_crossentropy', optimizer= adam,
                  metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])
    # summarize defined model
    model.summary()
    plot_model(model, to_file='model_bi_gru.png', show_shapes=True)
    return model

# BIGRU with FastText
def build_model_bi_gru_fastText(gru_units, k_regularizer,learning_rate,
                          dropout_rate, recurrent_dropout, emb_dim):
    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32)
    embedding_sequences = Embedding(input_dim = vocabulary_size  + 1,
                          embeddings_initializer= Constant(fastText_embedding_matrix),
                          trainable = False,
                          output_dim = emb_dim)(sequence_input)

    spatial_dropout = (SpatialDropout1D(dropout_rate))(embedding_sequences)
    bi_gru_1 = Bidirectional(GRU(units = gru_units, kernel_regularizer = l1(k_regularizer),
                   recurrent_dropout = recurrent_dropout, return_sequences = True))(spatial_dropout)
    # bi_gru_2 = Bidirectional(GRU(int(gru_units/2), return_sequences=True))(spatial_dropout)
    # concatenated_1 = concatenate([bi_gru_1, bi_gru_2])

    max_pooling = GlobalMaxPooling1D()(bi_gru_1)
    attention = AttentionLayer(MAX_SEQUENCE_LENGTH)(bi_gru_1)

    concatenated_2 = concatenate([max_pooling, attention])

    dense = Dense(gru_units, activation='relu')(concatenated_2)
    dropout = Dropout(dropout_rate)(dense)
    output = Dense(1, activation='sigmoid')(dropout)

    model = Model(inputs=sequence_input, outputs=output, name = 'model_bigru_fastText')
    # compile network
    adam = Adam(learning_rate = learning_rate)
    model.compile(loss='binary_crossentropy', optimizer= adam,
                  metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])
    # summarize defined model
    model.summary()
    plot_model(model, to_file='model_bi_gru_fastText300D.png', show_shapes=True)
    return model

# BIGRU with Glove
def build_model_bi_gru_glove(gru_units, k_regularizer,learning_rate,
                          dropout_rate, recurrent_dropout, emb_dim):
    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32)
    embedding_sequences = Embedding(input_dim = vocabulary_size  + 1,
                          embeddings_initializer= Constant(embedding_matrix_glove),
                          trainable = False,
                          output_dim = emb_dim)(sequence_input)
    spatial_dropout = (SpatialDropout1D(dropout_rate))(embedding_sequences)
    bi_gru = Bidirectional(GRU(units = gru_units, kernel_regularizer = l1(k_regularizer),
                   recurrent_dropout = recurrent_dropout, return_sequences = True))(spatial_dropout)
    max_pooling = GlobalMaxPooling1D()(bi_gru)
    attention = AttentionLayer(MAX_SEQUENCE_LENGTH)(bi_gru)
    concatenated = concatenate([max_pooling, attention])

    dense = Dense(gru_units, activation='relu')(concatenated)
    dropout = Dropout(dropout_rate)(dense)
    output = Dense(1, activation='sigmoid')(dropout)
    model = Model(inputs=sequence_input, outputs=output, name = 'model_bigru_glove')
    # compile network
    adam = Adam(learning_rate = learning_rate)
    model.compile(loss='binary_crossentropy', optimizer= adam,
                  metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])
    # summarize defined model
    model.summary()
    plot_model(model, to_file='model_bi_gru_glove300D.png', show_shapes=True)
    return model

"""##<h4> *BiGRU+CNN implementations* </h4>"""

MAX_SEQUENCE_LENGTH = 100
vocabulary_size = text_vector.vocabulary_size()
# BIGRU + CNN
# def build_model_bi_gru_cnn(filters, kernel_size, gru_units, k_regularizer,
#                            dense_units, learning_rate, dropout_rate,
#                            recurrent_dropout, emb_dim):
#     sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32)
#     embedding_sequences = Embedding(input_dim = vocabulary_size  + 1,
#                           output_dim = emb_dim)(sequence_input)
#     spatial_dropout = (SpatialDropout1D(dropout_rate))(embedding_sequences)
#     bi_gru = Bidirectional(GRU(units = gru_units, kernel_regularizer = l1(k_regularizer),
#                    recurrent_dropout = recurrent_dropout, return_sequences = True))(spatial_dropout)

#     conv_1d = Conv1D(filters, kernel_size, activation='relu')(spatial_dropout_1)
#     conv_2d = Conv1D(int(filters/2), kernel_size, activation='relu')(spatial_dropout_1)
#     conv_3d = Conv1D(int(filters/4), kernel_size, activation='relu')(spatial_dropout_1)
#     conv_4d = Conv1D(int(filters/8), kernel_size, activation='relu')(spatial_dropout_1)

#     concatenated = concatenate([conv_1d, conv_2d,
#                                 conv_3d, conv_4d])
#     max_pooling = GlobalMaxPooling1D()(concatenated)
#     attention = AttentionLayer(MAX_SEQUENCE_LENGTH)(spatial_dropout_1)
#     concatenated_1 = concatenate([max_pooling, attention])

#     dense = Dense(dense_units, activation='relu')(concatenated_1)
#     dropout = Dropout(dropout_rate)(dense)
#     output = Dense(1, activation='sigmoid')(dropout)
#     model = Model(inputs=sequence_input, outputs=output, name = 'model_bigru_cnn')
#     # compile network
#     adam = Adam(learning_rate = learning_rate)
#     model.compile(loss='binary_crossentropy', optimizer= adam,
#                   metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])
#     # summarize defined model
#     model.summary()
#     plot_model(model, to_file='model_bigru_cnn.png', show_shapes=True)
#     return model

def build_model_bi_gru_cnn_fastText(filters, kernel_size, gru_units, k_regularizer,
                           dense_units, learning_rate, dropout_rate,
                           recurrent_dropout, emb_dim):
    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32)
    embedding_sequences = Embedding(input_dim = vocabulary_size  + 1,
                          embeddings_initializer= Constant(fastText_embedding_matrix),
                          trainable = False,
                          output_dim = emb_dim)(sequence_input)
    spatial_dropout = (SpatialDropout1D(dropout_rate))(embedding_sequences)
    bi_gru_1 = Bidirectional(GRU(units = gru_units, kernel_regularizer = l1(k_regularizer),
                   recurrent_dropout = recurrent_dropout, return_sequences = True))(spatial_dropout)

    conv_1d = Conv1D(filters, kernel_size, activation='relu')(spatial_dropout)
    conv_2d = Conv1D(int(filters/2), kernel_size, activation='relu')(spatial_dropout)
    conv_3d = Conv1D(int(filters/4), kernel_size, activation='relu')(spatial_dropout)
    conv_4d = Conv1D(int(filters/8), kernel_size, activation='relu')(spatial_dropout)

    concatenated = concatenate([conv_1d, conv_2d,
                                conv_3d, conv_4d])
    max_pooling = GlobalMaxPooling1D()(concatenated)
    attention = AttentionLayer(MAX_SEQUENCE_LENGTH)( bi_gru_1)
    concatenated_1 = concatenate([max_pooling, attention])

    dense = Dense(dense_units, activation='relu')(concatenated_1)
    dropout = Dropout(dropout_rate)(dense)
    output = Dense(1, activation='sigmoid')(dropout)
    model = Model(inputs=sequence_input, outputs=output, name = 'model_bi_gru_cnn_fastText')
    # compile network
    adam = Adam(learning_rate = learning_rate)
    model.compile(loss='binary_crossentropy', optimizer= adam,
                  metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])
    # summarize defined model
    model.summary()
    plot_model(model, to_file='model_bigru_cnn_fastText300D.png', show_shapes=True)
    return model

def build_model_bi_gru_cnn_glove(filters, kernel_size, gru_units, k_regularizer,
                           dense_units, learning_rate, dropout_rate,
                           recurrent_dropout, emb_dim):
    sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32)
    embedding_sequences = Embedding(input_dim = vocabulary_size  + 1,
                          embeddings_initializer= Constant(embedding_matrix_glove),
                          trainable = False,
                          output_dim = emb_dim)(sequence_input)
    spatial_dropout = (SpatialDropout1D(dropout_rate))(embedding_sequences)
    bi_gru_1 = Bidirectional(GRU(units = gru_units, kernel_regularizer = l1(k_regularizer),
                   recurrent_dropout = recurrent_dropout, return_sequences = True))(spatial_dropout)

    conv_1d = Conv1D(filters, kernel_size, activation='relu')(spatial_dropout)
    conv_2d = Conv1D(int(filters/2), kernel_size, activation='relu')(spatial_dropout)
    conv_3d = Conv1D(int(filters/4), kernel_size, activation='relu')(spatial_dropout)
    conv_4d = Conv1D(int(filters/8), kernel_size, activation='relu')(spatial_dropout)

    concatenated = concatenate([conv_1d, conv_2d,
                                conv_3d, conv_4d])
    max_pooling = GlobalMaxPooling1D()(concatenated)
    attention = AttentionLayer(MAX_SEQUENCE_LENGTH)(bi_gru_1)
    concatenated_1 = concatenate([max_pooling, attention])

    dense = Dense(dense_units, activation='relu')(concatenated_1)
    dropout = Dropout(dropout_rate)(dense)
    output = Dense(1, activation='sigmoid')(dropout)
    model = Model(inputs=sequence_input, outputs=output, name = 'model_bi_gru_cnn_glove')
    # compile network
    adam = Adam(learning_rate = learning_rate)
    model.compile(loss='binary_crossentropy', optimizer= adam,
                  metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])
    # summarize defined model
    model.summary()
    plot_model(model, to_file='model_bigru_cnn_glove300D.png', show_shapes=True)
    return model

"""##<h4> *RoBERTa implementations* </h4>"""

MAX_SEQUENCE_LENGTH = 100
def build_model_roberta(gru_units, k_regularizer,
                           dense_units, dropout_rate,
                           recurrent_dropout, emb_dim):


    input_word_ids = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_word_ids')
    input_mask = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_mask')
    input_type_ids = tf.keras.Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_type_ids')

    # Import RoBERTa model from HuggingFace
    model = TFBertForSequenceClassification.from_pretrained("ydshieh/bert-base-uncased-yelp-polarity")

    roberta_model = TFRobertaModel.from_pretrained("roberta-base")
    #roberta_model =  TFRobertaForSequenceClassification.from_pretrained("roberta-base")
    roberta_model_final = roberta_model(input_word_ids, attention_mask = input_mask,  token_type_ids=input_type_ids, output_hidden_states=True)
    # The last hidden-state is the first element of the output tuple
    #spatial_dropout_0 = (SpatialDropout1D(dropout_rate))(roberta_model_final[0])

    # Add BiGRU_CNN
    bi_gru = Bidirectional(GRU(units = gru_units, kernel_regularizer = l1(k_regularizer),
                recurrent_dropout = recurrent_dropout, return_sequences = True))((roberta_model_final[0]))
    spatial_dropout_1 = (SpatialDropout1D(dropout_rate))(bi_gru)

    # conv_1d = Conv1D(filters, kernel_size, activation='relu')(spatial_dropout_1)
    # conv_2d = Conv1D(int(filters/2), kernel_size, activation='relu')(spatial_dropout_1)
    # conv_3d = Conv1D(int(filters/4), kernel_size, activation='relu')(spatial_dropout_1)
    # conv_4d = Conv1D(int(filters/8), kernel_size, activation='relu')(spatial_dropout_1)

    # concatenated = concatenate([conv_1d, conv_2d,
    #                             conv_3d, conv_4d])
    max_pooling = GlobalMaxPooling1D()(spatial_dropout_1)
    attention = AttentionLayer(MAX_SEQUENCE_LENGTH)(spatial_dropout_1)
    concatenated_1 = concatenate([max_pooling, attention])

    dense = Dense(dense_units, activation='relu')(concatenated_1)
    dropout = Dropout(dropout_rate)(dense)
    output = Dense(1, activation='sigmoid')(dropout)

    # dropout = Dropout(0.1)(roberta_model_final[0])
    # flattened = Flatten()(dropout)
    # # avg_pool = GlobalAveragePooling1D()(roberta_model_final[0])
    # # dropout = Dropout(0.3)(avg_pool)
    # dense = Dense(32, activation='relu')(flattened)
    # output = tf.keras.layers.Dense(1, activation="sigmoid")(dense)

    model = tf.keras.models.Model(inputs = [input_word_ids,  input_mask, input_type_ids],
    outputs = output, name = 'model_roberta')

    # Huggingface transformers have multiple outputs, embeddings are the first one,
    # so let's slice out the first position
    # x = x[0]

    # x = tf.keras.layers.Dropout(0.1)(x)
    # x = tf.keras.layers.Flatten()(x)
    # x = tf.keras.layers.Dense(256, activation='relu')(x)
    # x = tf.keras.layers.Dense(1, activation='sigmoid')(x)

    # model = tf.keras.Model(inputs=[input_word_ids, input_mask, input_type_ids], outputs=x, name = 'model_roberta')
    # compile network
    model.compile(
        optimizer=tf.keras.optimizers.Adam(lr=LEARNING_RATE),
        loss='binary_crossentropy',
        metrics=['accuracy',tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])
    # summarize defined model
    model.summary()
    plot_model(model, to_file='model_roberta.png', show_shapes=True)
    return model

"""## <h3> *Training with BART* </h3>"""

# import evaluate

# accuracy = evaluate.load("accuracy")

tokenizer = BartTokenizer.from_pretrained("facebook/bart-large-mnli")

def tokenize(sentences, tokenizer):
    input_ids, input_masks, input_segments = [],[],[]
    for sentence in sentences:
        inputs = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=100, pad_to_max_length=True, return_attention_mask=True, return_token_type_ids=False)
        input_ids.append(inputs['input_ids'])
        input_masks.append(inputs['attention_mask'])
        # input_segments.append(inputs['token_type_ids'])
        # np.asarray(input_segments, dtype='int32')

    return np.asarray(input_ids, dtype='int32'), np.asarray(input_masks, dtype='int32')

def build_classifier_model():
   input_ids_in = tf.keras.layers.Input(shape=(100,), name='input_token', dtype='int32')
   input_masks_in = tf.keras.layers.Input(shape=(100,), name='masked_token', dtype='int32')

   bart_model = TFBartForSequenceClassification.from_pretrained("facebook/bart-large-mnli")
   embedding_layer = bart_model(input_ids_in, attention_mask=input_masks_in)[0]
   X = tf.keras.layers.Dense(1, activation='sigmoid')(embedding_layer)

   model = tf.keras.Model(inputs=[input_ids_in, input_masks_in], outputs=X)

   for layer in model.layers[:3]:
        layer.trainable = False

   model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])
   model.summary()
   return model

X_train_bart = tokenize(X_train, tokenizer)
X_dev_bart = tokenize(X_dev, tokenizer)
X_test_bart = tokenize(X_test, tokenizer)

bart_model = build_classifier_model()
bart_model_history = fit_network(bart_model, X_train_bart, y_train, X_dev_bart, y_dev)

# # Preprocess X_train

# X_train_encoded = tokenizer([i for i in X_train.values], padding='max_length', max_length= 100, truncation=True, return_tensors='tf')

# # Preprocess X_dev
# X_dev_encoded = tokenizer([i for i in X_dev.values], padding= 'max_length',  max_length= 100, truncation=True, return_tensors='tf')

# # Preprocess X_test
# X_test_encoded = tokenizer([i for i in X_test.values], padding='max_length',  max_length= 100, truncation=True, return_tensors='tf')

# def compute_metrics(eval_pred):
#     predictions, labels = eval_pred
#     predictions = np.argmax(predictions, axis=1)
#     return accuracy.compute(predictions=predictions, references=labels)

# id2label = {0: "HATE", 1: "NO-HATE"}
# label2id = {"HATE": 0, "NO-HATE": 1}

# data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors="tf")

# id2label = {0: "NEGATIVE", 1: "POSITIVE"}
# label2id = {"NEGATIVE": 0, "POSITIVE": 1}

# batch_size = 16
# num_epochs = 5
# batches_per_epoch = len(X_train_encoded.data.get('input_ids')) // batch_size
# total_train_steps = int(batches_per_epoch * num_epochs)
# optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)



# bart_model.compile(optimizer=optimizer)

# from transformers.keras_callbacks import KerasMetricCallback

# metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset= (X_dev_encoded, y_dev), batch_size=64)

# from transformers.keras_callbacks import PushToHubCallback

# push_to_hub_callback = PushToHubCallback(
#     output_dir="my_awesome_model",
#     tokenizer=tokenizer,
# )

# callbacks = [metric_callback]

# bart_model.fit(X_train_encoded,y_train, validation_data=(X_dev_encoded, y_dev), epochs=3, callbacks=callbacks)

"""## <h3> *Training on all data* </h3>

### <h4> *CNN models* </h4>
"""

cnn_model_fastText_300_a = build_model_cnn_fastText(filters = 512, kernel_size=6,
                                              dropout_rate= 0.5,
                                              dense_units= 512, emb_dim = 300)
cnn_model_fastText_history_300_a = fit_network(cnn_model_fastText_300_a,
                                          X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(cnn_model_fastText_history_300_a)

from keras.models import load_model
saved_model_cnn_f_o = load_model('model_cnn_fastText.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_cnn_f_o, X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_cnn_f_o, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss, test_precision*100, test_recall*100, test_f1_score*100))

get_auc_score(saved_model_cnn_f_o, X_test_dl, y_test)

draw_confusion_matrix(saved_model_cnn_f_o, X_test_dl, y_test)

draw_auprc_score(saved_model_cnn_f_o, X_test_dl, y_test)

draw_roc_curve(saved_model_cnn_f_o, X_test_dl, y_test)

cnn_model_glove_300_a = build_model_cnn_glove(filters = 512, kernel_size=6,
                                              dropout_rate= 0.5,
                                              dense_units= 512, emb_dim = 300)
cnn_model_glove_history_300_a = fit_network(cnn_model_glove_300_a,
                                          X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(cnn_model_glove_history_300_a)

from keras.models import load_model
saved_model_cnn_g_o = load_model('model_cnn_glove.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_cnn_g_o, X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_cnn_g_o, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss, test_precision*100, test_recall*100, test_f1_score*100))

get_auc_score(saved_model_cnn_g_o, X_test_dl, y_test)

draw_confusion_matrix(saved_model_cnn_g_o, X_test_dl, y_test)

draw_auprc_score(saved_model_cnn_g_o, X_test_dl, y_test)

draw_roc_curve(saved_model_cnn_g_o, X_test_dl, y_test)

"""### <h4> *LSTM models* </h4>"""

lstm_model_fastText_300_a = build_model_lstm_fastText(lstm_units = 64, dense_units = 512,
                              k_regularizer = 0.001, dropout_rate = 0.3,
                              recurrent_dropout = 0.0, emb_dim = 300)
lstm_fastText_history_300_a = fit_network(lstm_model_fastText_300_a, X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(lstm_fastText_history_300_a)

from keras.models import load_model
saved_model_lstm_f_a = load_model('model_lstm_fastText.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_lstm_f_a , X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_lstm_f_a, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss, test_precision*100, test_recall*100, test_f1_score*100))

get_auc_score(saved_model_lstm_f_a, X_test_dl, y_test)

draw_confusion_matrix(saved_model_lstm_f_a, X_test_dl, y_test)

draw_auprc_score(saved_model_lstm_f_a, X_test_dl, y_test)

draw_roc_curve(saved_model_lstm_f_a, X_test_dl, y_test)

lstm_model_glove_300_a = build_model_lstm_glove(lstm_units = 64, dense_units = 512,
                              k_regularizer = 0.001, dropout_rate = 0.3,
                              recurrent_dropout = 0.0, emb_dim = 300)
lstm_glove_history_300_a = fit_network(lstm_model_glove_300_a, X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(lstm_glove_history_300_a)

from keras.models import load_model
saved_model_lstm_g_a = load_model('model_lstm_glove.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_lstm_g_a , X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_lstm_g_a, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss, test_precision*100, test_recall*100, test_f1_score*100))

get_auc_score(saved_model_lstm_g_a, X_test_dl, y_test)

draw_confusion_matrix(saved_model_lstm_g_a, X_test_dl, y_test)

draw_auprc_score(saved_model_lstm_g_a, X_test_dl, y_test)

draw_roc_curve(saved_model_lstm_g_a, X_test_dl, y_test)

"""### <h4> *GRU models* </h4>"""

gru_model_fastText_300_a = build_model_gru_fastText(gru_units = 64,learning_rate = 0.001,
                              k_regularizer = 0.00001, dropout_rate = 0.5,
                              recurrent_dropout = 0.0, emb_dim = 300)
gru_fastText_history_300_a = fit_network(gru_model_fastText_300_a, X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(gru_fastText_history_300_a)

from keras.models import load_model
saved_model_gru_f_a = load_model('model_gru_fastText.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_gru_f_a , X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_gru_f_a, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss, test_precision*100, test_recall*100, test_f1_score*100))

get_auc_score(saved_model_gru_f_a, X_test_dl, y_test)

draw_confusion_matrix(saved_model_gru_f_a, X_test_dl, y_test)

draw_auprc_score(saved_model_gru_f_a, X_test_dl, y_test)

draw_roc_curve(saved_model_gru_f_a, X_test_dl, y_test)

gru_model_glove_300_a = build_model_gru_glove(gru_units = 64 ,learning_rate = 0.001,
                              k_regularizer = 0.00001, dropout_rate = 0.5,
                              recurrent_dropout = 0.0, emb_dim = 300)
gru_glove_history_300_a = fit_network(gru_model_glove_300_a, X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(gru_glove_history_300_a)

from keras.models import load_model
saved_model_gru_g_a = load_model('model_gru_glove.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_gru_g_a , X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_gru_g_a, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss, test_precision*100, test_recall*100, test_f1_score*100))

get_auc_score(saved_model_gru_g_a, X_test_dl, y_test)

draw_confusion_matrix(saved_model_gru_g_a, X_test_dl, y_test)

draw_auprc_score(saved_model_gru_g_a, X_test_dl, y_test)

draw_roc_curve(saved_model_gru_g_a, X_test_dl, y_test)

"""### <h4> *BiGRU models* </h4>"""

bi_gru_model_fastText_a = build_model_bi_gru_fastText(gru_units = 256, k_regularizer = 0.00001,learning_rate = 0.001,
                                dropout_rate = 0.5, recurrent_dropout= 0.0, emb_dim = 300)
bi_gru_model_fastText_history_a = fit_network(bi_gru_model_fastText_a, X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(bi_gru_model_fastText_history_a)

from keras.models import load_model
saved_model_bi_gru_f_a = load_model('model_bigru_fastText.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_bi_gru_f_a , X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_bi_gru_f_a, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss, test_precision*100, test_recall*100, test_f1_score*100))

get_auc_score(saved_model_bi_gru_f_a, X_test_dl, y_test)

draw_confusion_matrix(saved_model_bi_gru_f_a, X_test_dl, y_test)

draw_auprc_score(saved_model_bi_gru_f_a, X_test_dl, y_test)

draw_roc_curve(saved_model_bi_gru_f_a, X_test_dl, y_test)

bi_gru_model_glove_300_a = build_model_bi_gru_glove(gru_units = 64, k_regularizer = 0.00001,learning_rate= 0.001,
                                dropout_rate = 0.4, recurrent_dropout= 0.0, emb_dim = 300)
bi_gru_model_glove_history_300_a = fit_network(bi_gru_model_glove_300_a , X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(bi_gru_model_glove_history_300_a)

from keras.models import load_model
saved_model_bi_gru_g_a = load_model('model_bigru_glove.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_bi_gru_g_a, X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_bi_gru_g_a, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss, test_precision*100, test_recall*100, test_f1_score*100))

get_auc_score(saved_model_bi_gru_g_a, X_test_dl, y_test)

draw_confusion_matrix(saved_model_bi_gru_g_a, X_test_dl, y_test)

draw_auprc_score(saved_model_bi_gru_g_a, X_test_dl, y_test)

draw_roc_curve(saved_model_bi_gru_g_a, X_test_dl, y_test)

"""### <h4> *BiGRU + CNN models* </h4>"""

bi_gru_cnn_model_fastText_a = build_model_bi_gru_cnn_fastText(filters = 16, kernel_size=6,
                                              dropout_rate= 0.5, dense_units= 64,
                                              gru_units = 256, k_regularizer = 0.00001,
                                              learning_rate= 0.003, recurrent_dropout= 0.0, emb_dim = 300)
bi_gru_cnn_model_fastText_history_a = fit_network(bi_gru_cnn_model_fastText_a , X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(bi_gru_cnn_model_fastText_history_a)

from keras.models import load_model
saved_model_bi_gru_cnn_f_a = load_model('model_bi_gru_cnn_fastText.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_bi_gru_cnn_f_a , X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_bi_gru_cnn_f_a, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f, Train precision: %.2f, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f, Test precision: %.2f, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss, test_precision*100, test_recall*100, test_f1_score*100))

get_auc_score(saved_model_bi_gru_cnn_f_a, X_test_dl, y_test)

draw_confusion_matrix(saved_model_bi_gru_cnn_f_a, X_test_dl, y_test)

draw_auprc_score(saved_model_bi_gru_cnn_f_a, X_test_dl, y_test)

draw_roc_curve(saved_model_bi_gru_cnn_f_a, X_test_dl, y_test)

bi_gru_cnn_model_glove_300_a = build_model_bi_gru_cnn_glove(filters = 8, kernel_size=6,
                                              dropout_rate= 0.4, dense_units= 64,
                                              gru_units = 256, k_regularizer = 0.00001,
                                              learning_rate= 0.003, recurrent_dropout= 0.0, emb_dim = 300)
bi_gru_cnn_model_glove_history_300_a = fit_network(bi_gru_cnn_model_glove_300_a , X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(bi_gru_cnn_model_glove_history_300_a)

from keras.models import load_model
saved_model_bi_gru_cnn_g_a = load_model('model_bi_gru_cnn_glove.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_bi_gru_cnn_g_a , X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_bi_gru_cnn_g_a, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss, test_precision*100, test_recall*100, test_f1_score*100))

get_auc_score(saved_model_bi_gru_cnn_g_a, X_test_dl, y_test)

draw_confusion_matrix(saved_model_bi_gru_cnn_g_a, X_test_dl, y_test)

draw_auprc_score(saved_model_bi_gru_cnn_g_a, X_test_dl, y_test)

draw_roc_curve(saved_model_bi_gru_cnn_g_a, X_test_dl, y_test)

"""### <h4> *BiGRU + LSTM models* </h4>

### <h4> *RoBERTA models* </h4>
"""

roberta_model_a = build_model_roberta(gru_units = 64, k_regularizer = 0.00001,
                                     dropout_rate = 0.5, dense_units = 256,
                                     recurrent_dropout = 0.0, emb_dim = 300)
roberta_history_a = fit_network(roberta_model_a, X_train_roberta, y_train, X_dev_roberta, y_dev)

plot_accuracy_loss(roberta_history_a)

get_auc_score(roberta_history_a, X_test_roberta, y_test)

from keras.models import load_model
saved_model_roberta_a = load_model('model_roberta.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_roberta_a , X_train_roberta, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_roberta_a, X_test_roberta, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss, test_precision*100, test_recall*100, test_f1_score*100))

draw_confusion_matrix(saved_model_roberta_a, X_test_roberta, y_test)

draw_auprc_score(saved_model_roberta_a, X_test_roberta, y_test)

draw_roc_curve(saved_model_roberta_a, X_test_roberta, y_test)

"""## <h3> *Training on perturbated data* </h3>

### <h4> *CNN models* </h4>
"""

cnn_model_fastText_300 = build_model_cnn_fastText(filters = 512, kernel_size=6,
                                              dropout_rate= 0.5,
                                              dense_units= 512, emb_dim = 300)
cnn_model_fastText_history_300 = fit_network(cnn_model_fastText_300,
                                          X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(cnn_model_fastText_history_300)

from keras.models import load_model
saved_model_cnn_f = load_model('model_cnn_fastText.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_cnn_f, X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_cnn_f, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss, test_precision*100, test_recall*100, test_f1_score*100))

get_auc_score(saved_model_cnn_f, X_test_dl, y_test)

draw_confusion_matrix(saved_model_cnn_f, X_test_dl, y_test)

draw_auprc_score(saved_model_cnn_f, X_test_dl, y_test)

draw_roc_curve(saved_model_cnn_f, X_test_dl, y_test)

cnn_model_glove_300 = build_model_cnn_glove(filters = 512, kernel_size=6,
                                              dropout_rate= 0.5,
                                              dense_units= 512, emb_dim = 300)
cnn_model_glove_history_300 = fit_network(cnn_model_glove_300,
                                          X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(cnn_model_glove_history_300)

from keras.models import load_model
saved_model_cnn_g = load_model('model_cnn_glove.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_cnn_g , X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_cnn_g, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss, test_precision*100, test_recall*100, test_f1_score*100))

get_auc_score(saved_model_cnn_g, X_test_dl,y_test)

draw_confusion_matrix(saved_model_cnn_g, X_test_dl, y_test)

draw_auprc_score(saved_model_cnn_g, X_test_dl, y_test)

draw_roc_curve(saved_model_cnn_g, X_test_dl, y_test)

"""### <h4> *LSTM models* </h4>"""

lstm_model_fastText_300 = build_model_lstm_fastText(lstm_units = 64, dense_units = 512,
                              k_regularizer = 0.001, dropout_rate = 0.3,
                              recurrent_dropout = 0.0, emb_dim = 300)
lstm_fastText_history_300 = fit_network(lstm_model_fastText_300, X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(lstm_fastText_history_300)

from keras.models import load_model
saved_model_lstm_f = load_model('model_lstm_fastText.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_lstm_f , X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_lstm_f, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss, test_precision*100, test_recall*100, test_f1_score*100))

get_auc_score(saved_model_lstm_f, X_test_dl, y_test)

draw_confusion_matrix(saved_model_lstm_f, X_test_dl, y_test)

draw_auprc_score(saved_model_lstm_f, X_test_dl, y_test)

draw_roc_curve(saved_model_lstm_f, X_test_dl, y_test)

lstm_model_glove_300 = build_model_lstm_glove(lstm_units = 64, dense_units = 512,
                              k_regularizer = 0.001, dropout_rate = 0.3,
                              recurrent_dropout = 0.0, emb_dim = 300)
lstm_glove_history_300 = fit_network(lstm_model_glove_300, X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(lstm_glove_history_300)

from keras.models import load_model
saved_model_lstm_g = load_model('model_lstm_glove.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_lstm_g , X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_lstm_g, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss, test_precision*100, test_recall*100, test_f1_score*100))

get_auc_score(saved_model_lstm_g, X_test_dl, y_test)

draw_confusion_matrix(saved_model_lstm_g, X_test_dl, y_test)

draw_auprc_score(saved_model_lstm_g, X_test_dl, y_test)

draw_roc_curve(saved_model_lstm_g, X_test_dl, y_test)

"""### <h4> *GRU models* </h4>"""

gru_model_fastText_300 = build_model_gru_fastText(gru_units = 64,learning_rate = 0.001,
                              k_regularizer = 0.00001, dropout_rate = 0.5,
                              recurrent_dropout = 0.0, emb_dim = 300)
gru_fastText_history_300 = fit_network(gru_model_fastText_300, X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(gru_fastText_history_300)

from keras.models import load_model
saved_model_gru_f = load_model('model_gru_fastText.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_gru_f , X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_gru_f, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss, test_precision*100, test_recall*100, test_f1_score*100))

get_auc_score(saved_model_gru_f, X_test_dl, y_test)

draw_confusion_matrix(saved_model_gru_f, X_test_dl, y_test)

draw_auprc_score(saved_model_gru_f, X_test_dl, y_test)

draw_roc_curve(saved_model_gru_f, X_test_dl, y_test)

gru_model_glove_300 = build_model_gru_glove(gru_units = 64 ,learning_rate = 0.001,
                              k_regularizer = 0.00001, dropout_rate = 0.5,
                              recurrent_dropout = 0.0, emb_dim = 300)
gru_glove_history_300 = fit_network(gru_model_glove_300, X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(gru_glove_history_300)

from keras.models import load_model
saved_model_gru_g = load_model('model_gru_glove.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_gru_g , X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_gru_g, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss, test_precision*100, test_recall*100, test_f1_score*100))

get_auc_score(saved_model_gru_g, X_test_dl, y_test)

draw_confusion_matrix(saved_model_gru_g, X_test_dl, y_test)

draw_auprc_score(saved_model_gru_g, X_test_dl, y_test)

draw_roc_curve(saved_model_gru_g, X_test_dl, y_test)

"""### <h4> *Bi-GRU models* </h4>"""

bi_gru_model_fastText = build_model_bi_gru_fastText(gru_units = 256, k_regularizer = 0.00001,learning_rate = 0.001,
                                dropout_rate = 0.5, recurrent_dropout= 0.0, emb_dim = 300)
bi_gru_model_fastText_history = fit_network(bi_gru_model_fastText, X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(bi_gru_model_fastText_history)

from keras.models import load_model
saved_model_bi_gru_f = load_model('model_bigru_fastText.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_bi_gru_f , X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_bi_gru_f, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss, test_precision*100, test_recall*100, test_f1_score*100))

get_auc_score(saved_model_bi_gru_f, X_test_dl, y_test)

draw_confusion_matrix(saved_model_bi_gru_f, X_test_dl, y_test)

draw_auprc_score(saved_model_bi_gru_f, X_test_dl, y_test)

draw_roc_curve(saved_model_bi_gru_f, X_test_dl, y_test)

bi_gru_model_glove_300 = build_model_bi_gru_glove(gru_units = 64, k_regularizer = 0.00001,learning_rate= 0.001,
                                dropout_rate = 0.4, recurrent_dropout= 0.0, emb_dim = 300)
bi_gru_model_glove_history_300 = fit_network(bi_gru_model_glove_300 , X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(bi_gru_model_glove_history_300)

from keras.models import load_model
saved_model_bi_gru_g = load_model('model_bigru_glove.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_bi_gru_g , X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_bi_gru_g, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss, test_precision*100, test_recall*100, test_f1_score*100))

get_auc_score(saved_model_bi_gru_g, X_test_dl, y_test)

draw_confusion_matrix(saved_model_bi_gru_g, X_test_dl, y_test)

draw_auprc_score(saved_model_bi_gru_g, X_test_dl, y_test)

draw_roc_curve(saved_model_bi_gru_g, X_test_dl, y_test)

"""### <h4> *Bi-GRU + CNN models* </h4>"""

bi_gru_cnn_model_fastText_ = build_model_bi_gru_cnn_fastText(filters = 16, kernel_size=6,
                                              dropout_rate= 0.5, dense_units = 64,
                                              gru_units = 256, k_regularizer = 0.00001,
                                              learning_rate= 0.003, recurrent_dropout= 0.0, emb_dim = 300)
bi_gru_cnn_model_fastText_history_1 = fit_network(bi_gru_cnn_model_fastText_ , X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(bi_gru_cnn_model_fastText_history_1)

from keras.models import load_model
saved_model_bi_gru_cnn_f = load_model('model_bi_gru_cnn_fastText.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_bi_gru_cnn_f , X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_bi_gru_cnn_f, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss, test_precision*100, test_recall*100, test_f1_score*100))

# draw_roc_curve(saved_model, X_test_dl):

get_auc_score(saved_model_bi_gru_cnn_f, X_test_dl, y_test)

draw_confusion_matrix(saved_model_bi_gru_cnn_f, X_test_dl, y_test)

draw_auprc_score(saved_model_bi_gru_cnn_f, X_test_dl, y_test)

draw_roc_curve(saved_model_bi_gru_cnn_f, X_test_dl, y_test)

bi_gru_cnn_model_glove_300 = build_model_bi_gru_cnn_glove(filters = 8, kernel_size=6,
                                              dropout_rate = 0.5, dense_units= 64,
                                              gru_units = 256, k_regularizer = 0.00001,
                                              learning_rate= 0.003, recurrent_dropout= 0.0, emb_dim = 300)
bi_gru_cnn_model_glove_history_300 = fit_network(bi_gru_cnn_model_glove_300 , X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(bi_gru_cnn_model_glove_history_300)

from keras.models import load_model
saved_model_bi_gru_cnn_g = load_model('model_bi_gru_cnn_glove.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_bi_gru_cnn_g , X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_bi_gru_cnn_g, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss, test_precision*100, test_recall*100, test_f1_score*100))

get_auc_score(saved_model_bi_gru_cnn_g, X_test_dl, y_test)

draw_confusion_matrix(saved_model_bi_gru_cnn_g, X_test_dl, y_test)

draw_auprc_score(saved_model_bi_gru_cnn_g, X_test_dl, y_test)

draw_roc_curve(saved_model_bi_gru_cnn_g, X_test_dl, y_test)

"""### <h4> *Bi-GRU + LSTM models* </h4>"""

bi_gru_lstm_model_fastText_ = build_model_bi_gru_lstm_fastText(gru_units = 128, lstm_units = 64, dense_units = 512 , k_regularizer = 0.00001, learning_rate = 0.001,
                                dropout_rate = 0.5, recurrent_dropout= 0.0, emb_dim = 300)
bi_gru_lstm_model_fastText_history = fit_network(bi_gru_lstm_model_fastText_, X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(bi_gru_lstm_model_fastText_history)

from keras.models import load_model
saved_model_bi_gru_lstm_f = load_model('model_bigru_lstm_fastText.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_bi_gru_lstm_f , X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_bi_gru_lstm_f, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss, test_precision*100, test_recall*100, test_f1_score*100))

get_auc_score(saved_model_bi_gru_lstm_f, X_test_dl, y_test)

draw_confusion_matrix(saved_model_bi_gru_lstm_f, X_test_dl, y_test)

draw_auprc_score(saved_model_bi_gru_lstm_f, X_test_dl, y_test)

bi_gru_lstm_model_glove_ = build_model_bi_gru_lstm_glove(gru_units = 32, lstm_units = 32, dense_units = 32 , k_regularizer = 0.00001, learning_rate = 0.001,
                                dropout_rate = 0.5, recurrent_dropout= 0.0, emb_dim = 300)
bi_gru_lstm_model_glove_history = fit_network(bi_gru_lstm_model_glove_, X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(bi_gru_lstm_model_glove_history)

from keras.models import load_model
saved_model_bi_gru_lstm_g = load_model('model_bigru_lstm_glove.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_bi_gru_lstm_g , X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_bi_gru_lstm_g, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss, test_precision*100, test_recall*100, test_f1_score*100))

draw_confusion_matrix(saved_model_bi_gru_lstm_g, X_test_dl, y_test)

get_auc_score(saved_model_bi_gru_lstm_g, X_test_dl, y_test)

"""### <h4> *RoBERTA + BiGRU models* </h4>"""

# roberta_model = build_model_roberta( dropout_rate = 0.5, dense_units = 64, gru_units = 64, k_regularizer = 0.00001, recurrent_dropout = 0.0, emb_dim = 300)
# roberta_history = fit_network(roberta_model, X_train_roberta, y_train, X_dev_roberta, y_dev)

plot_accuracy_loss(roberta_history)

from keras.models import load_model
saved_model_roberta_bi_gru_ = load_model('model_roberta.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_roberta_bi_gru_ , X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_roberta_bi_gru_, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss, test_precision*100, test_recall*100, test_f1_score*100))

draw_roc_curve(roberta_model, X_test_roberta)

draw_confusion_matrix(roberta_model, X_test_roberta, y_test)

"""### <h4> *Facebook NLI Model* </h4>"""

# nli_model = build_model_bart(filters = 64, kernel_size=6,
#                                               dropout_rate= 0.5, dense_units= 64,
#                                               gru_units = 512, k_regularizer = 0.00001,
#                                               learning_rate= 0.003, recurrent_dropout= 0.0, emb_dim = 300)
# roberta_history = fit_network(roberta_model, X_train_facebook_bart, y_train, X_dev_facebook_bart, y_dev)

"""## <h3> *Training on original data* </h3>

### <h4> *CNN models* </h4>
"""

# cnn_model_ = build_model_cnn(filters = 256, kernel_size = 6, dropout_rate = 0.5,
#                              dense_units = 256, emb_dim = 100)
# cnn_history = fit_network(cnn_model_, X_train_dl, y_train, X_dev_dl, y_dev)

# plot_accuracy_loss(cnn_history)

# cnn_model_.save('cnn_model.keras')

cnn_model_fastText_o = build_model_cnn_fastText(filters = 256, kernel_size=6,
                                              dropout_rate= 0.5,
                                              dense_units= 256, emb_dim = 300)
cnn_model_fastText_history_o = fit_network(cnn_model_fastText_o,
                                          X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(cnn_model_fastText_history_o)

from keras.models import load_model
saved_model_cnn_fastText = load_model('model_cnn_fastText.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_cnn_fastText, X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_cnn_fastText, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f%%, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss*100, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f%%, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss*100, test_precision*100, test_recall*100, test_f1_score*100))

# draw_roc_curve(saved_model_cnn_fastText, X_test_dl)

draw_auprc_score(saved_model_cnn_fastText, X_test_dl, y_test) # AUC PRC score

get_auc_score(saved_model_cnn_fastText, X_test_dl, y_test) # AUC ROC score

draw_confusion_matrix(saved_model_cnn_fastText, X_test_dl, y_test)

cnn_model_glove_o = build_model_cnn_glove(filters = 256, kernel_size=6,
                                              dropout_rate= 0.5,
                                              dense_units= 256, emb_dim = 300)
cnn_model_glove_history_o = fit_network(cnn_model_glove_o,
                                          X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(cnn_model_glove_history_o)

from keras.models import load_model
saved_model_cnn_glove = load_model('model_cnn_glove.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_cnn_glove, X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_cnn_glove, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f%%, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss*100, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f%%, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss*100, test_precision*100, test_recall*100, test_f1_score*100))

# draw_roc_curve(saved_model_cnn_glove, X_test_dl)

draw_auprc_score(saved_model_cnn_glove, X_test_dl, y_test) # AUC PRC score

get_auc_score(saved_model_cnn_glove, X_test_dl, y_test) # AUC ROC score

draw_confusion_matrix(saved_model_cnn_glove, X_test_dl, y_test)

"""### <h4> *LSTM models* </h4>"""

# lstm_model_ = build_model_lstm(lstm_units = 64, dense_units = 512,
#                               k_regularizer = 0.001,
#                               dropout_rate = 0.3, recurrent_dropout = 0.0,emb_dim = 100)
# lstm_history = fit_network(lstm_model_, X_train_dl, y_train, X_dev_dl, y_dev)

lstm_model_fastText_o = build_model_lstm_fastText(lstm_units = 64, dense_units = 512,
                              k_regularizer = 0.001, dropout_rate = 0.3,
                              recurrent_dropout = 0.0, emb_dim = 300)
lstm_fastText_history_o = fit_network(lstm_model_fastText_o, X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(lstm_fastText_history_o)

from keras.models import load_model
saved_model_lstm_fastText = load_model('model_lstm_fastText.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_lstm_fastText, X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_lstm_fastText, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f%%, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss*100, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f%%, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss*100, test_precision*100, test_recall*100, test_f1_score*100))

draw_auprc_score(saved_model_lstm_fastText, X_test_dl, y_test) # AUC PRC score

get_auc_score(saved_model_lstm_fastText, X_test_dl, y_test) # AUC ROC score

draw_confusion_matrix(saved_model_lstm_fastText, X_test_dl, y_test)

lstm_model_glove_o = build_model_lstm_glove(lstm_units = 64, dense_units = 512,
                              k_regularizer = 0.001, dropout_rate = 0.3,
                              recurrent_dropout = 0.0, emb_dim = 300)
lstm_glove_history_o = fit_network(lstm_model_glove_o, X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(lstm_glove_history_o)

from keras.models import load_model
saved_model_lstm_glove = load_model('model_lstm_glove.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_lstm_glove, X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_lstm_glove, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f%%, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss*100, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f%%, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss*100, test_precision*100, test_recall*100, test_f1_score*100))

draw_auprc_score(saved_model_lstm_glove, X_test_dl, y_test) # AUC PRC score

get_auc_score(saved_model_lstm_glove, X_test_dl, y_test) # AUC ROC score

draw_confusion_matrix(saved_model_lstm_glove, X_test_dl, y_test)

"""### <h4> *GRU models* </h4>"""

# gru_model = build_model_gru(gru_units = 64,learning_rate = 0.003,
#                               k_regularizer = 0.00001, dropout_rate = 0.4,
#                               recurrent_dropout = 0.0, emb_dim = 100)
# gru_history = fit_network(gru_model, X_train_dl, y_train, X_dev_dl, y_dev)

gru_model_fastText_o = build_model_gru_fastText(gru_units = 64,learning_rate = 0.003,
                              k_regularizer = 0.00001, dropout_rate = 0.4,
                              recurrent_dropout = 0.0, emb_dim = 300)
gru_fastText_history_o = fit_network(gru_model_fastText_o, X_train_dl, y_train, X_dev_dl, y_dev)

from keras.models import load_model
saved_model_gru_fastText = load_model('model_gru_fastText.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_gru_fastText, X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_gru_fastText, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f%%, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss*100, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f%%, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss*100, test_precision*100, test_recall*100, test_f1_score*100))

plot_accuracy_loss(gru_fastText_history_o)

draw_auprc_score(saved_model_gru_fastText, X_test_dl, y_test) # AUC PRC score

get_auc_score(saved_model_gru_fastText, X_test_dl, y_test) # AUC ROC score

draw_confusion_matrix(saved_model_gru_fastText, X_test_dl, y_test)

gru_model_glove_o = build_model_gru_glove(gru_units = 64,learning_rate = 0.003,
                              k_regularizer = 0.00001, dropout_rate = 0.4,
                              recurrent_dropout = 0.0, emb_dim = 300)
gru_glove_history_o = fit_network(gru_model_glove_o, X_train_dl, y_train, X_dev_dl, y_dev)

from keras.models import load_model
saved_model_gru_glove = load_model('model_gru_glove.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_gru_glove, X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_gru_glove, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f%%, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss*100, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f%%, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss*100, test_precision*100, test_recall*100, test_f1_score*100))

draw_auprc_score(saved_model_gru_glove, X_test_dl, y_test) # AUC PRC score

get_auc_score(saved_model_gru_glove, X_test_dl, y_test) # AUC ROC score

draw_confusion_matrix(saved_model_gru_glove, X_test_dl, y_test)

plot_accuracy_loss(gru_glove_history_o)

"""### <h4> *Bi-GRU models* </h4>"""

# bi_gru_model_ = build_model_bi_gru(gru_units = 64, k_regularizer = 0.00001,learning_rate = 0.003,
#                                 dropout_rate = 0.4, recurrent_dropout= 0.0, emb_dim = 100)
# bi_gru_model_history = fit_network(bi_gru_model_, X_train_dl, y_train, X_dev_dl, y_dev)

bi_gru_model_fastText_o = build_model_bi_gru_fastText(gru_units = 64, k_regularizer = 0.00001,learning_rate = 0.003,
                                dropout_rate = 0.4, recurrent_dropout= 0.0, emb_dim = 300)
bi_gru_model_fastText_history_o = fit_network(bi_gru_model_fastText_o, X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(bi_gru_model_fastText_history_o)

from keras.models import load_model
saved_model_bi_gru_fastText = load_model('model_bigru_fastText.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_bi_gru_fastText, X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_bi_gru_fastText, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f%%, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss*100, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f%%, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss*100, test_precision*100, test_recall*100, test_f1_score*100))

draw_auprc_score(saved_model_bi_gru_fastText, X_test_dl, y_test) # AUC PRC score

get_auc_score(saved_model_bi_gru_fastText, X_test_dl, y_test) # AUC ROC score

draw_confusion_matrix(saved_model_bi_gru_fastText, X_test_dl, y_test)

bi_gru_model_glove_o = build_model_bi_gru_glove(gru_units = 64, k_regularizer = 0.00001,learning_rate= 0.003,
                                dropout_rate = 0.4, recurrent_dropout= 0.0, emb_dim = 300)
bi_gru_model_glove_history_o = fit_network(bi_gru_model_glove_o , X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(bi_gru_model_glove_history_o)

from keras.models import load_model
saved_model_bi_gru_glove = load_model('model_bigru_glove.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_bi_gru_glove, X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_bi_gru_glove, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f%%, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss*100, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f%%, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss*100, test_precision*100, test_recall*100, test_f1_score*100))

draw_auprc_score(saved_model_bi_gru_glove, X_test_dl, y_test) # AUC PRC score

get_auc_score(saved_model_bi_gru_glove, X_test_dl, y_test) # AUC ROC score

draw_confusion_matrix(saved_model_bi_gru_glove, X_test_dl, y_test)

"""### <h4> *BiGRU + CNN models* </h4>"""

# bi_gru_cnn_model = build_model_bi_gru_cnn(filters = 64, kernel_size=6,
#                                               dropout_rate= 0.5, dense_units= 256,
#                                               gru_units = 64, k_regularizer = 0.00001,
#                                               learning_rate= 0.003, recurrent_dropout= 0.0, emb_dim = 100)
# bi_gru_cnn_model_history = fit_network(bi_gru_cnn_model, X_train_dl, y_train, X_dev_dl, y_dev)

bi_gru_cnn_model_fastText_o = build_model_bi_gru_cnn_fastText(filters = 64, kernel_size=6,
                                              dropout_rate= 0.5, dense_units= 256,
                                              gru_units = 512, k_regularizer = 0.00001,
                                              learning_rate= 0.003, recurrent_dropout= 0.0, emb_dim = 300)
bi_gru_cnn_model_fastText_history_o = fit_network(bi_gru_cnn_model_fastText_o , X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(bi_gru_cnn_model_fastText_history_o)

from keras.models import load_model
saved_model_bi_gru_cnn_fastText = load_model('model_bi_gru_cnn_fastText.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_bi_gru_cnn_fastText, X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_bi_gru_cnn_fastText, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f%%, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss*100, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f%%, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss*100, test_precision*100, test_recall*100, test_f1_score*100))

draw_auprc_score(saved_model_bi_gru_cnn_fastText, X_test_dl, y_test) # AUC PRC score

get_auc_score(saved_model_bi_gru_cnn_fastText, X_test_dl, y_test) # AUC ROC score

draw_confusion_matrix(saved_model_bi_gru_cnn_fastText, X_test_dl, y_test)

bi_gru_cnn_model_glove_o = build_model_bi_gru_cnn_glove(filters = 64, kernel_size=6,
                                              dropout_rate= 0.5, dense_units= 256,
                                              gru_units = 512, k_regularizer = 0.00001,
                                              learning_rate= 0.003, recurrent_dropout= 0.0, emb_dim = 300)
bi_gru_cnn_model_glove_history_o = fit_network(bi_gru_cnn_model_glove_o , X_train_dl, y_train, X_dev_dl, y_dev)

plot_accuracy_loss(bi_gru_cnn_model_glove_history_o)

from keras.models import load_model
saved_model_bi_gru_cnn_glove = load_model('model_bi_gru_cnn_glove.keras', custom_objects={'AttentionLayer': AttentionLayer})
train_loss, train_acc, train_precision, train_recall, train_f1_score = evaluate_model(saved_model_bi_gru_cnn_glove, X_train_dl, y_train)
test_loss, test_acc, test_precision, test_recall, test_f1_score = evaluate_model(saved_model_bi_gru_cnn_glove, X_test_dl, y_test)
print('Train accuracy: %.2f%%, Train loss: %.2f%%, Train precision: %.2f%%, Train recall: %.2f%%, Train f1-score: %.2f%%' % (train_acc*100, train_loss*100, train_precision*100, train_recall*100, train_f1_score*100))
print('Test accuracy: %.2f%%, Test loss: %.2f%%, Test precision: %.2f%%, Test recall: %.2f%%, Test f1-score: %.2f%%' % (test_acc*100, test_loss*100, test_precision*100, test_recall*100, test_f1_score*100))

draw_auprc_score(saved_model_bi_gru_cnn_glove, X_test_dl, y_test)

get_auc_score(saved_model_bi_gru_cnn_glove, X_test_dl, y_test)

draw_confusion_matrix(saved_model_bi_gru_cnn_glove, X_test_dl, y_test)

"""### <h4> *RoBERTA + BiGRU + CNN models* </h4>"""

roberta_model_o = build_model_roberta(filters = 64, kernel_size=6,
                                              dropout_rate= 0.5, dense_units= 256,
                                              gru_units = 512, k_regularizer = 0.00001,
                                              learning_rate= 0.003, recurrent_dropout= 0.0, emb_dim = 300)
roberta_history_o = fit_network(roberta_model_o, X_train_roberta, y_train, X_dev_roberta, y_dev)

"""## <h3> *Hyperparameters tuning* </h3>"""

# def tune_hyperparameters_cnn(X_train, y_train, X_val, y_val):
#     # build_model_cnn_emb
#     keras_model = KerasClassifier(build_fn = build_model_cnn, dense_units = 256,
#                                 dropout_rate = 0.5, filters = 256, kernel_size=6,
#                                 activation ='relu',verbose = 3)
#     # define grid parameters
#     param_grid = {
#             #'batch_size' : [16, 32, 64, 128, 256],
#             #'epochs' : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
#             #'activation' : ['relu', 'tanh', 'sigmoid'],
#             }
#     grid = GridSearchCV(estimator = keras_model, param_grid = param_grid,
#                         cv = 3, verbose = 3, error_score='raise')
#     grid_result = grid.fit(X_train, y_train, validation_data = (X_val, y_val))
#     # summarize results
#     print(f" Best result is : {grid_result.best_score_} using : {grid_result.best_params_}")
#     best_model = grid_result.best_estimator_
#     return best_model

# def tune_hyperparameters_lstm(X_train, y_train, X_val, y_val):
#     # build_model_lstm_emb
#     keras_model = KerasClassifier(build_fn = build_model_lstm, lstm_units = 64,
#                                   dense_units = 512, activation = 'relu',
#                                   recurrent_activation = 'sigmoid',
#                                   dropout_rate = 0.3, recurrent_dropout = 0.0,
#                                   k_regularizer = 0.001,
#                                   verbose = 3)
#     # define grid parameters
#     param_grid = {
#             # 'activation' : ['relu', 'tanh', 'sigmoid'],
#             #'recurrent_activation' : ['relu', 'tanh', 'sigmoid'],
#             # 'lstm_units' : [32, 64, 128, 256, 512],
#             # 'dropout_rate' : [0.0, 0.2, 0.3, 0.4, 0.5],
#             # 'dense_units' : [32, 64, 128, 256, 512],
#             # 'recurrent_dropout': [ 0.0, 0.2, 0.3, 0.4],
#             # 'k_regularizer':[0.00001, 0.0001, 0.001, 0.01, 0.1],
#             #'optimizer' : [Adam(learning_rate = 0.00001),
#                   #Adam(learning_rate = 0.00003), Adam(learning_rate = 0.0001),
#                   #Adam(learning_rate = 0.0003), Adam(learning_rate = 0.001),
#                   #Adam(learning_rate = 0.003), Adam(learning_rate = 0.01),
#                   #Adam(learning_rate = 0.03), Adam(learning_rate = 0.1),
#                   #Adam(learning_rate = 0.3)],
#             #'batch_size' : [16, 32, 64, 128, 256],
#             #'epochs' : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
#     }
#     grid = GridSearchCV(estimator = keras_model, param_grid = param_grid,
#                         cv = 3, verbose = 3)
#     grid_result = grid.fit(X_train, y_train, validation_data = (X_val, y_val))
#     # summarize results
#     print(f" Best result is : {grid_result.best_score_} using : {grid_result.best_params_}")
#     best_model = grid_result.best_estimator_
#     return best_model

# def tune_hyperparameters_gru(X_train, y_train, X_val, y_val):
#     # build_model_gru_emb
#     keras_model = KerasClassifier(build_fn = build_model_gru, gru_units = 32,
#                                   dropout_rate = 0.4, recurrent_dropout = 0.0,
#                                   learning_rate = 0.003,
#                                   k_regularizer = 0.00001,
#                                   verbose = 3)
#     # define grid parameters
#     param_grid = {
#             # 'gru_units' : [32, 64, 128, 256, 512],
#             # 'dropout_rate' : [0.0, 0.2, 0.3, 0.4, 0.5],
#             # 'recurrent_dropout': [ 0.0, 0.2, 0.3, 0.4],
#             # 'dense_units' : [32, 64, 128, 256, 512],
#             # 'learning_rate':[0.00003, 0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03,
#             #                  0.1, 0.3]
#             #'k_regularizer':[0.00001, 0.0001, 0.001, 0.01, 0.1],
#             # 'opt':['Adam', 'RMSprop', 'SGD', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam'],
#             # 'optimizer' : [Adam(learning_rate = 0.00001),
#             #       Adam(learning_rate = 0.00003), Adam(learning_rate = 0.0001),
#             #       Adam(learning_rate = 0.0003), Adam(learning_rate = 0.001),
#             #       Adam(learning_rate = 0.003), Adam(learning_rate = 0.01),
#             #       Adam(learning_rate = 0.03), Adam(learning_rate = 0.1),
#             #       Adam(learning_rate = 0.3)],
#             #'batch_size' : [16, 32, 64, 128, 256], 64
#             'epochs' : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
#     }
#     grid = GridSearchCV(estimator = keras_model, param_grid = param_grid,
#                         cv = 3, verbose = 3)
#     grid_result = grid.fit(X_train, y_train, validation_data = (X_val, y_val))
#     # summarize results
#     print(f" Best result is : {grid_result.best_score_} using : {grid_result.best_params_}")
#     best_model = grid_result.best_estimator_
#     return best_model

# tune_hyperparameters_cnn(X_train_dl, y_train, X_dev_dl, y_dev)

# tune_hyperparameters_lstm(X_train_dl, y_train, X_dev_dl, y_dev)

# tune_hyperparameters_gru(X_train_dl, y_train, X_dev_dl, y_dev)

classifier = transformers.pipeline("zero-shot-classification", model="facebook/bart-large-mnli")

text = "I enjoy playing cricket, specializing as a left-arm leg spinner while showcasing my skills as a right-handed one-down batsman."
labels = ['Politics', 'Automobile', 'Sports', 'Business', 'World']

prediction = classifier(text, labels)

print(prediction['sequence'])
print(prediction['labels'])
print(prediction['scores'])